{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayTjan/Roost/blob/main/Roost_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6194f6c8"
      },
      "source": [
        "# Text Retrieval"
      ],
      "id": "6194f6c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fssaul-bWSF2"
      },
      "source": [
        "### Fetching"
      ],
      "id": "Fssaul-bWSF2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBKn3zMMkxGf",
        "outputId": "9351cf44-d2a4-4065-9363-e7f932661321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install kaggle"
      ],
      "id": "kBKn3zMMkxGf"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zn1m0iODk0Hs"
      },
      "outputs": [],
      "source": [
        "mkdir ~/.kaggle"
      ],
      "id": "Zn1m0iODk0Hs"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-DPyIV2dk2uM"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "id": "-DPyIV2dk2uM"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g9bSeEM2k6gI"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "g9bSeEM2k6gI"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWf5BbwflGM-",
        "outputId": "89fd04e6-d8a0-4f06-ed7d-1c8bb46f21dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading suicide-watch.zip to /content\n",
            " 69% 42.0M/60.6M [00:00<00:00, 51.8MB/s]\n",
            "100% 60.6M/60.6M [00:00<00:00, 77.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download nikhileswarkomati/suicide-watch"
      ],
      "id": "AWf5BbwflGM-"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yB6DgcWlU3h",
        "outputId": "734813cc-3993-4647-9877-e1a349d21865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  suicide-watch.zip\n",
            "  inflating: Suicide_Detection.csv   \n"
          ]
        }
      ],
      "source": [
        "! unzip suicide-watch.zip"
      ],
      "id": "2yB6DgcWlU3h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ckDHLEWUUs"
      },
      "source": [
        "### Exploratory Analysis"
      ],
      "id": "b4ckDHLEWUUs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dwJi_QHpV9bP",
        "outputId": "003cc912-fc22-4693-9526-6cdf39c19869"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-af364f80-a2f8-467d-819f-c0e3e6c35752\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
              "      <td>suicide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Am I weird I don't get affected by compliments...</td>\n",
              "      <td>non-suicide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
              "      <td>non-suicide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>i need helpjust help me im crying so hard</td>\n",
              "      <td>suicide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
              "      <td>suicide</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af364f80-a2f8-467d-819f-c0e3e6c35752')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af364f80-a2f8-467d-819f-c0e3e6c35752 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af364f80-a2f8-467d-819f-c0e3e6c35752');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0                                               text        class\n",
              "0           2  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
              "1           3  Am I weird I don't get affected by compliments...  non-suicide\n",
              "2           4  Finally 2020 is almost over... So I can never ...  non-suicide\n",
              "3           8          i need helpjust help me im crying so hard      suicide\n",
              "4           9  I’m so lostHello, my name is Adam (16) and I’v...      suicide"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/Suicide_Detection.csv')\n",
        "df.head()"
      ],
      "id": "dwJi_QHpV9bP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "bb45ffea",
        "outputId": "b2b93efd-9a8e-4ce6-ecf9-369b3790b64c",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 232074 entries, 0 to 232073\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count   Dtype \n",
            "---  ------      --------------   ----- \n",
            " 0   Unnamed: 0  232074 non-null  int64 \n",
            " 1   text        232074 non-null  object\n",
            " 2   class       232074 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 5.3+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-84917984-71db-4892-b133-7928ed8512ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>232074.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>174152.863518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>100500.425362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>87049.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>174358.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>261285.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>348110.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84917984-71db-4892-b133-7928ed8512ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-84917984-71db-4892-b133-7928ed8512ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-84917984-71db-4892-b133-7928ed8512ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          Unnamed: 0\n",
              "count  232074.000000\n",
              "mean   174152.863518\n",
              "std    100500.425362\n",
              "min         2.000000\n",
              "25%     87049.250000\n",
              "50%    174358.500000\n",
              "75%    261285.750000\n",
              "max    348110.000000"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df.info()\n",
        "df.describe()"
      ],
      "id": "bb45ffea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "a11b2242",
        "outputId": "f30c2ba0-f44b-48a7-9b66-04c2a4fdae9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2811071c-f238-41f7-b443-f94f5ae955ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232069</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232070</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232071</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232072</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232073</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>232074 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2811071c-f238-41f7-b443-f94f5ae955ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2811071c-f238-41f7-b443-f94f5ae955ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2811071c-f238-41f7-b443-f94f5ae955ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Unnamed: 0   text  class\n",
              "0            False  False  False\n",
              "1            False  False  False\n",
              "2            False  False  False\n",
              "3            False  False  False\n",
              "4            False  False  False\n",
              "...            ...    ...    ...\n",
              "232069       False  False  False\n",
              "232070       False  False  False\n",
              "232071       False  False  False\n",
              "232072       False  False  False\n",
              "232073       False  False  False\n",
              "\n",
              "[232074 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.isna()"
      ],
      "id": "a11b2242"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "299e337a",
        "outputId": "ad6ebf4a-dc8b-40c5-d1f6-195726d31810"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.isna().values.any()"
      ],
      "id": "299e337a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4654f0",
        "outputId": "5bfce454-85ba-4851-9cad-f42937186038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['suicide', 'non-suicide']\n"
          ]
        }
      ],
      "source": [
        "target_names = list(df['class'].unique())\n",
        "\n",
        "print(target_names)"
      ],
      "id": "9d4654f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31dcfb66",
        "outputId": "b18ec40e-a8d1-40c3-dff8-f31b553ac573"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "non-suicide    116037\n",
              "suicide        116037\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df['class'].value_counts()"
      ],
      "id": "31dcfb66"
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = np.where(df['class'] == \"suicide\", 1, 0)\n",
        "df = df.drop(['Unnamed: 0'], axis=1)"
      ],
      "metadata": {
        "id": "RQkx5DxO8L1p"
      },
      "id": "RQkx5DxO8L1p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "R51ijJ7Y8X1E",
        "outputId": "11dff224-f25c-47a6-a8bf-d8809a468a3c"
      },
      "id": "R51ijJ7Y8X1E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-03de60b6-c373-47b9-92cf-30b10508a677\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
              "      <td>suicide</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Am I weird I don't get affected by compliments...</td>\n",
              "      <td>non-suicide</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
              "      <td>non-suicide</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i need helpjust help me im crying so hard</td>\n",
              "      <td>suicide</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
              "      <td>suicide</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03de60b6-c373-47b9-92cf-30b10508a677')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03de60b6-c373-47b9-92cf-30b10508a677 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03de60b6-c373-47b9-92cf-30b10508a677');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text        class  label\n",
              "0  Ex Wife Threatening SuicideRecently I left my ...      suicide      1\n",
              "1  Am I weird I don't get affected by compliments...  non-suicide      0\n",
              "2  Finally 2020 is almost over... So I can never ...  non-suicide      0\n",
              "3          i need helpjust help me im crying so hard      suicide      1\n",
              "4  I’m so lostHello, my name is Adam (16) and I’v...      suicide      1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKb2WtdhLJ6K",
        "outputId": "3ffe243d-67c1-41c9-9012-f5cfea46abac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /root/.local/lib/python3.7/site-packages (3.6.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /root/.local/lib/python3.7/site-packages (from nltk) (2021.11.10)\n"
          ]
        }
      ],
      "source": [
        "pip install --user -U nltk"
      ],
      "id": "nKb2WtdhLJ6K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NSuQ5luZZX"
      },
      "source": [
        "### Text Pre-processing"
      ],
      "id": "e5NSuQ5luZZX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utl-0BxXGHWO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Selecting 50% of dataset for training and test set with 4:1 proportion\n",
        "df_samples = df.sample(frac=0.5, replace=True, random_state=10)\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_samples['text'], df_samples['label'], test_size=0.2, random_state=10)"
      ],
      "id": "Utl-0BxXGHWO"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xRO_QfPeJeHR"
      },
      "id": "xRO_QfPeJeHR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPOtuiMiuuH7",
        "outputId": "e9b1f244-6bbb-4569-b502-b3a728ed5275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Removes url in text\n",
        "def remove_url(text):\n",
        "  clean_text = re.sub(r'(http|ftp|ssh|www)\\S+', '', text)\n",
        "  return clean_text\n",
        "\n",
        "# Removes emoji in text\n",
        "def remove_emoji(text):\n",
        "  emoji_pattern = re.compile(pattern=\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Set stop words\n",
        "def set_stop_words():\n",
        "  stop_words_nltk = set(stopwords.words('english'))\n",
        "  punctuations = set(string.punctuation)\n",
        "  stop_words_extra = {'cannot', 'filler', 'wa'}\n",
        "  stop_words = stop_words_nltk.union(stop_words_extra, punctuations)\n",
        "  return stop_words\n",
        "\n",
        "stop_words = set_stop_words()\n",
        "\n",
        "# Remove affixes (prefix and suffix) to return to root words\n",
        "def remove_affixes_tenses(text) :\n",
        "  tokens = TweetTokenizer().tokenize(text)\n",
        "  clean_text = ' '.join([WordNetLemmatizer().lemmatize(word, 'v') for word in tokens])\n",
        "  return clean_text\n",
        "\n",
        "# Defining corpus for English words from NLTK\n",
        "corpus_english = set(nltk.corpus.words.words())\n",
        "\n",
        "# Remove non-English words based on the defined corpus above\n",
        "def remove_non_english(text) :\n",
        "  clean_text = ' '.join(w for w in nltk.wordpunct_tokenize(text) \\\n",
        "          if w.lower() in corpus_english or not w.isalpha())\n",
        "  return clean_text\n",
        "\n",
        "# Remove punctuations\n",
        "def remove_punct(text):\n",
        "  clean_text = ''.join([char for char in text if char not in string.punctuation])\n",
        "  clean_text = re.sub('[0-9]+','', text)\n",
        "  return clean_text\n",
        "\n",
        "# Apply tokenization\n",
        "def tokenization(text):\n",
        "  clean_text = re.split('\\W+', text)\n",
        "  return clean_text\n",
        "\n",
        "# Removing stop words\n",
        "def remove_stop_words(text):\n",
        "  clean_text = [word for word in text if word not in stop_words]\n",
        "  return clean_text"
      ],
      "id": "oPOtuiMiuuH7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbm_Cs9evUg_",
        "outputId": "013bbf61-df3f-45b2-b83a-bb5d367577c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73592     [hello, people, want, ask, give, advice, talk,...\n",
              "169590    [server, ruin, log, survival, server, tree, to...\n",
              "109136    [seek, help, follow, sub, scan, talk, tip, wan...\n",
              "54104     [mad, confident, important, test, take, today,...\n",
              "203632    [anxiety, unbearable, spend, whole, day, mind,...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x_train = x_train.apply(lambda text: text.lower())\n",
        "x_train = x_train.apply(lambda text: remove_url(text))\n",
        "x_train = x_train.apply(lambda text: remove_emoji(text))\n",
        "x_train = x_train.apply(lambda text: remove_affixes_tenses(text))\n",
        "x_train = x_train.apply(lambda text: remove_non_english(text))\n",
        "x_train = x_train.apply(lambda text: remove_punct(text))\n",
        "x_train = x_train.apply(lambda text: tokenization(text))\n",
        "x_train = x_train.apply(lambda text: remove_stop_words(text))\n",
        "x_train.head()"
      ],
      "id": "Xbm_Cs9evUg_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGtD3V8UAQkH"
      },
      "source": [
        "# Stemming"
      ],
      "id": "JGtD3V8UAQkH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFyj2v_RAWzX"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Apply stemming\n",
        "\n",
        "stemmer = nltk.SnowballStemmer(language='english')\n",
        "\n",
        "def stemming(nonstop_words):\n",
        "  words = [stemmer.stem(word) for word in nonstop_words]\n",
        "  text = ' '.join(words)\n",
        "  return text\n",
        "\n",
        "x_train = x_train.apply(lambda text: stemming(text))"
      ],
      "id": "cFyj2v_RAWzX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRbkE08Td0_Y"
      },
      "source": [
        "# Frequency Measurement"
      ],
      "id": "gRbkE08Td0_Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijQ_mzGbWuHm"
      },
      "source": [
        "### Fetch Most Frequent Words"
      ],
      "id": "ijQ_mzGbWuHm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhhYNUQqbUAm"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_n_gram(corpus, ngram_range, max_features=None):\n",
        "  vectorizer = CountVectorizer(ngram_range=ngram_range,stop_words=stop_words,max_features=max_features).fit(corpus)\n",
        "  bag_of_words = vectorizer.transform(corpus)\n",
        "  sum_words = bag_of_words.sum(axis=0)\n",
        "  words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "  words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "  return words_freq"
      ],
      "id": "MhhYNUQqbUAm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbPkb5-ZL778"
      },
      "source": [
        "### 1-Gram CountVectorizer"
      ],
      "id": "NbPkb5-ZL778"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Apply 1-Gram CountVectorizer\n",
        "\n",
        "one_gram_fetch = get_n_gram(x_train, (1,1), 150)\n",
        "one_gram_df = pd.DataFrame(one_gram_fetch, columns=['Keyword', 'Frequency'])\n",
        "\n",
        "one_gram_df.info()\n",
        "one_gram_df.describe()\n",
        "one_gram_df.loc[:]\n",
        "plt.scatter(one_gram_df['Keyword'], one_gram_df['Frequency'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "4fdkeQAow8Fa",
        "outputId": "4ecc8cc4-b77e-4cc0-ee39-8a22eb842e0c"
      },
      "id": "4fdkeQAow8Fa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Keyword    150 non-null    object\n",
            " 1   Frequency  150 non-null    int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 2.5+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwBfh9LlM1oY",
        "outputId": "84983b56-cd23-4dc9-bea2-71a042e5a620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of extracted tokens\n",
            "['abl' 'actual' 'ago' 'alon' 'alreadi' 'also' 'alway' 'anoth' 'anyon'\n",
            " 'anyth' 'around' 'ask' 'away' 'back' 'bad' 'becom' 'best' 'better'\n",
            " 'break' 'call' 'care' 'chang' 'come' 'could' 'cri' 'dad' 'day' 'depress'\n",
            " 'die' 'dont' 'els' 'end' 'enough' 'even' 'ever' 'everi' 'everyon'\n",
            " 'everyth' 'famili' 'feel' 'felt' 'final' 'find' 'first' 'friend' 'get'\n",
            " 'girl' 'give' 'go' 'good' 'guy' 'happen' 'happi' 'hard' 'hate' 'help'\n",
            " 'home' 'hope' 'hurt' 'job' 'keep' 'kill' 'know' 'last' 'leav' 'let'\n",
            " 'life' 'like' 'littl' 'live' 'long' 'look' 'lose' 'lot' 'love' 'make'\n",
            " 'mani' 'mayb' 'mean' 'mental' 'might' 'mind' 'move' 'much' 'need' 'never'\n",
            " 'new' 'night' 'noth' 'old' 'one' 'pain' 'parent' 'past' 'peopl' 'person'\n",
            " 'place' 'plan' 'pleas' 'point' 'post' 'pretti' 'probabl' 'put' 'read'\n",
            " 'realli' 'reason' 'right' 'sad' 'say' 'scar' 'school' 'see' 'seem' 'self'\n",
            " 'sinc' 'sleep' 'someon' 'someth' 'sorri' 'start' 'still' 'stop' 'suicid'\n",
            " 'sure' 'take' 'talk' 'tell' 'thank' 'thing' 'think' 'though' 'time'\n",
            " 'tire' 'today' 'tri' 'two' 'understand' 'use' 'want' 'way' 'well' 'wish'\n",
            " 'without' 'work' 'world' 'wors' 'would' 'write' 'year']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abl': 0,\n",
              " 'actual': 1,\n",
              " 'ago': 2,\n",
              " 'alon': 3,\n",
              " 'alreadi': 4,\n",
              " 'also': 5,\n",
              " 'alway': 6,\n",
              " 'anoth': 7,\n",
              " 'anyon': 8,\n",
              " 'anyth': 9,\n",
              " 'around': 10,\n",
              " 'ask': 11,\n",
              " 'away': 12,\n",
              " 'back': 13,\n",
              " 'bad': 14,\n",
              " 'becom': 15,\n",
              " 'best': 16,\n",
              " 'better': 17,\n",
              " 'break': 18,\n",
              " 'call': 19,\n",
              " 'care': 20,\n",
              " 'chang': 21,\n",
              " 'come': 22,\n",
              " 'could': 23,\n",
              " 'cri': 24,\n",
              " 'dad': 25,\n",
              " 'day': 26,\n",
              " 'depress': 27,\n",
              " 'die': 28,\n",
              " 'dont': 29,\n",
              " 'els': 30,\n",
              " 'end': 31,\n",
              " 'enough': 32,\n",
              " 'even': 33,\n",
              " 'ever': 34,\n",
              " 'everi': 35,\n",
              " 'everyon': 36,\n",
              " 'everyth': 37,\n",
              " 'famili': 38,\n",
              " 'feel': 39,\n",
              " 'felt': 40,\n",
              " 'final': 41,\n",
              " 'find': 42,\n",
              " 'first': 43,\n",
              " 'friend': 44,\n",
              " 'get': 45,\n",
              " 'girl': 46,\n",
              " 'give': 47,\n",
              " 'go': 48,\n",
              " 'good': 49,\n",
              " 'guy': 50,\n",
              " 'happen': 51,\n",
              " 'happi': 52,\n",
              " 'hard': 53,\n",
              " 'hate': 54,\n",
              " 'help': 55,\n",
              " 'home': 56,\n",
              " 'hope': 57,\n",
              " 'hurt': 58,\n",
              " 'job': 59,\n",
              " 'keep': 60,\n",
              " 'kill': 61,\n",
              " 'know': 62,\n",
              " 'last': 63,\n",
              " 'leav': 64,\n",
              " 'let': 65,\n",
              " 'life': 66,\n",
              " 'like': 67,\n",
              " 'littl': 68,\n",
              " 'live': 69,\n",
              " 'long': 70,\n",
              " 'look': 71,\n",
              " 'lose': 72,\n",
              " 'lot': 73,\n",
              " 'love': 74,\n",
              " 'make': 75,\n",
              " 'mani': 76,\n",
              " 'mayb': 77,\n",
              " 'mean': 78,\n",
              " 'mental': 79,\n",
              " 'might': 80,\n",
              " 'mind': 81,\n",
              " 'move': 82,\n",
              " 'much': 83,\n",
              " 'need': 84,\n",
              " 'never': 85,\n",
              " 'new': 86,\n",
              " 'night': 87,\n",
              " 'noth': 88,\n",
              " 'old': 89,\n",
              " 'one': 90,\n",
              " 'pain': 91,\n",
              " 'parent': 92,\n",
              " 'past': 93,\n",
              " 'peopl': 94,\n",
              " 'person': 95,\n",
              " 'place': 96,\n",
              " 'plan': 97,\n",
              " 'pleas': 98,\n",
              " 'point': 99,\n",
              " 'post': 100,\n",
              " 'pretti': 101,\n",
              " 'probabl': 102,\n",
              " 'put': 103,\n",
              " 'read': 104,\n",
              " 'realli': 105,\n",
              " 'reason': 106,\n",
              " 'right': 107,\n",
              " 'sad': 108,\n",
              " 'say': 109,\n",
              " 'scar': 110,\n",
              " 'school': 111,\n",
              " 'see': 112,\n",
              " 'seem': 113,\n",
              " 'self': 114,\n",
              " 'sinc': 115,\n",
              " 'sleep': 116,\n",
              " 'someon': 117,\n",
              " 'someth': 118,\n",
              " 'sorri': 119,\n",
              " 'start': 120,\n",
              " 'still': 121,\n",
              " 'stop': 122,\n",
              " 'suicid': 123,\n",
              " 'sure': 124,\n",
              " 'take': 125,\n",
              " 'talk': 126,\n",
              " 'tell': 127,\n",
              " 'thank': 128,\n",
              " 'thing': 129,\n",
              " 'think': 130,\n",
              " 'though': 131,\n",
              " 'time': 132,\n",
              " 'tire': 133,\n",
              " 'today': 134,\n",
              " 'tri': 135,\n",
              " 'two': 136,\n",
              " 'understand': 137,\n",
              " 'use': 138,\n",
              " 'want': 139,\n",
              " 'way': 140,\n",
              " 'well': 141,\n",
              " 'wish': 142,\n",
              " 'without': 143,\n",
              " 'work': 144,\n",
              " 'world': 145,\n",
              " 'wors': 146,\n",
              " 'would': 147,\n",
              " 'write': 148,\n",
              " 'year': 149}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "one_gram_vectorizer = CountVectorizer(ngram_range=(1,1), max_features=150)\n",
        "one_gram_training_count = one_gram_vectorizer.fit_transform(x_train)\n",
        "print(\"List of extracted tokens\")\n",
        "print(one_gram_vectorizer.get_feature_names_out())\n",
        "one_gram_vectorizer.vocabulary_"
      ],
      "id": "SwBfh9LlM1oY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j5XLtjpdTFR",
        "outputId": "07d2f12d-a5c3-48e0-9514-5e7d0ec9296e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description of the word occurences data structure:\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "(Documents, Tokens)\n",
            "(92829, 150)\n",
            "Word occurrences of the first document:\n",
            "  (0, 94)\t4\n",
            "  (0, 139)\t2\n",
            "  (0, 11)\t1\n",
            "  (0, 47)\t2\n",
            "  (0, 126)\t2\n",
            "  (0, 105)\t1\n",
            "  (0, 33)\t3\n",
            "  (0, 62)\t2\n",
            "  (0, 6)\t1\n",
            "  (0, 63)\t1\n",
            "  (0, 45)\t1\n",
            "  (0, 140)\t1\n",
            "  (0, 72)\t1\n",
            "  (0, 44)\t1\n",
            "  (0, 48)\t1\n",
            "  (0, 52)\t1\n",
            "  (0, 117)\t1\n",
            "  (0, 23)\t1\n"
          ]
        }
      ],
      "source": [
        "print(\"Description of the word occurences data structure:\")\n",
        "print(type(one_gram_training_count))\n",
        "print(\"(Documents, Tokens)\")\n",
        "print(one_gram_training_count.shape)\n",
        "print(\"Word occurrences of the first document:\")\n",
        "print(one_gram_training_count[0])"
      ],
      "id": "2j5XLtjpdTFR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbzR6u60P0dB"
      },
      "source": [
        "###  2-Gram CountVectorizer"
      ],
      "id": "BbzR6u60P0dB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "suE-NbS_NheC",
        "outputId": "922e301c-5bab-45f3-8533-4d05a6851cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Keyword    150 non-null    object\n",
            " 1   Frequency  150 non-null    int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 2.5+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Apply 2-Gram CountVectorizer\n",
        "\n",
        "two_gram_fetch = get_n_gram(x_train, (2,2), 150)\n",
        "two_gram_df = pd.DataFrame(two_gram_fetch, columns=['Keyword', 'Frequency'])\n",
        "\n",
        "two_gram_df.info()\n",
        "two_gram_df.describe()\n",
        "two_gram_df.loc[:]\n",
        "plt.scatter(two_gram_df['Keyword'], two_gram_df['Frequency'])\n",
        "plt.show()"
      ],
      "id": "suE-NbS_NheC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl7rGA6PPH3K",
        "outputId": "79295645-2295-4022-c3ba-3ce982ddc18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of extracted tokens\n",
            "['act like' 'anyon els' 'attempt suicid' 'bacon bacon' 'best friend'\n",
            " 'cake cake' 'come back' 'commit suicid' 'cum cum' 'day ago' 'day day'\n",
            " 'depress anxieti' 'die let' 'die want' 'dont know' 'dont want' 'end life'\n",
            " 'entir life' 'even get' 'even know' 'even though' 'ever sinc' 'everi day'\n",
            " 'everi singl' 'everi time' 'everyon els' 'feel alon' 'feel bad'\n",
            " 'feel better' 'feel like' 'feel way' 'felt like' 'first time' 'get back'\n",
            " 'get better' 'get help' 'get job' 'get wors' 'go away' 'go back' 'go get'\n",
            " 'go go' 'go kill' 'go school' 'go sleep' 'good enough' 'high school'\n",
            " 'keep go' 'know feel' 'know get' 'know go' 'know know' 'know peopl'\n",
            " 'know want' 'know would' 'last night' 'last time' 'last year' 'let die'\n",
            " 'let go' 'life feel' 'life get' 'life go' 'life want' 'like get'\n",
            " 'like go' 'like one' 'like want' 'live life' 'live like' 'long time'\n",
            " 'look forward' 'look like' 'love love' 'make feel' 'make happi'\n",
            " 'make sens' 'make sure' 'make want' 'mani peopl' 'mani time'\n",
            " 'mental health' 'mental ill' 'need get' 'need help' 'need someon'\n",
            " 'never get' 'never go' 'never realli' 'next day' 'one care' 'one day'\n",
            " 'one thing' 'panic attack' 'peopl know' 'peopl like' 'peopl say'\n",
            " 'pleas help' 'pretti much' 'realli bad' 'realli know' 'realli want'\n",
            " 'say want' 'seem like' 'self harm' 'someon els' 'someon talk'\n",
            " 'someth like' 'still feel' 'suicid attempt' 'take care' 'take life'\n",
            " 'talk peopl' 'th grade' 'thank read' 'think get' 'think go' 'think kill'\n",
            " 'think suicid' 'think would' 'time get' 'time go' 'treat like' 'tri get'\n",
            " 'tri hard' 'tri help' 'tri kill' 'tri make' 'ur ur' 'video game'\n",
            " 'want die' 'want end' 'want feel' 'want get' 'want go' 'want help'\n",
            " 'want hurt' 'want kill' 'want know' 'want live' 'want make' 'want see'\n",
            " 'want someon' 'want stop' 'want talk' 'want want' 'whole life'\n",
            " 'wish could' 'would like' 'year old']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'act like': 0,\n",
              " 'anyon els': 1,\n",
              " 'attempt suicid': 2,\n",
              " 'bacon bacon': 3,\n",
              " 'best friend': 4,\n",
              " 'cake cake': 5,\n",
              " 'come back': 6,\n",
              " 'commit suicid': 7,\n",
              " 'cum cum': 8,\n",
              " 'day ago': 9,\n",
              " 'day day': 10,\n",
              " 'depress anxieti': 11,\n",
              " 'die let': 12,\n",
              " 'die want': 13,\n",
              " 'dont know': 14,\n",
              " 'dont want': 15,\n",
              " 'end life': 16,\n",
              " 'entir life': 17,\n",
              " 'even get': 18,\n",
              " 'even know': 19,\n",
              " 'even though': 20,\n",
              " 'ever sinc': 21,\n",
              " 'everi day': 22,\n",
              " 'everi singl': 23,\n",
              " 'everi time': 24,\n",
              " 'everyon els': 25,\n",
              " 'feel alon': 26,\n",
              " 'feel bad': 27,\n",
              " 'feel better': 28,\n",
              " 'feel like': 29,\n",
              " 'feel way': 30,\n",
              " 'felt like': 31,\n",
              " 'first time': 32,\n",
              " 'get back': 33,\n",
              " 'get better': 34,\n",
              " 'get help': 35,\n",
              " 'get job': 36,\n",
              " 'get wors': 37,\n",
              " 'go away': 38,\n",
              " 'go back': 39,\n",
              " 'go get': 40,\n",
              " 'go go': 41,\n",
              " 'go kill': 42,\n",
              " 'go school': 43,\n",
              " 'go sleep': 44,\n",
              " 'good enough': 45,\n",
              " 'high school': 46,\n",
              " 'keep go': 47,\n",
              " 'know feel': 48,\n",
              " 'know get': 49,\n",
              " 'know go': 50,\n",
              " 'know know': 51,\n",
              " 'know peopl': 52,\n",
              " 'know want': 53,\n",
              " 'know would': 54,\n",
              " 'last night': 55,\n",
              " 'last time': 56,\n",
              " 'last year': 57,\n",
              " 'let die': 58,\n",
              " 'let go': 59,\n",
              " 'life feel': 60,\n",
              " 'life get': 61,\n",
              " 'life go': 62,\n",
              " 'life want': 63,\n",
              " 'like get': 64,\n",
              " 'like go': 65,\n",
              " 'like one': 66,\n",
              " 'like want': 67,\n",
              " 'live life': 68,\n",
              " 'live like': 69,\n",
              " 'long time': 70,\n",
              " 'look forward': 71,\n",
              " 'look like': 72,\n",
              " 'love love': 73,\n",
              " 'make feel': 74,\n",
              " 'make happi': 75,\n",
              " 'make sens': 76,\n",
              " 'make sure': 77,\n",
              " 'make want': 78,\n",
              " 'mani peopl': 79,\n",
              " 'mani time': 80,\n",
              " 'mental health': 81,\n",
              " 'mental ill': 82,\n",
              " 'need get': 83,\n",
              " 'need help': 84,\n",
              " 'need someon': 85,\n",
              " 'never get': 86,\n",
              " 'never go': 87,\n",
              " 'never realli': 88,\n",
              " 'next day': 89,\n",
              " 'one care': 90,\n",
              " 'one day': 91,\n",
              " 'one thing': 92,\n",
              " 'panic attack': 93,\n",
              " 'peopl know': 94,\n",
              " 'peopl like': 95,\n",
              " 'peopl say': 96,\n",
              " 'pleas help': 97,\n",
              " 'pretti much': 98,\n",
              " 'realli bad': 99,\n",
              " 'realli know': 100,\n",
              " 'realli want': 101,\n",
              " 'say want': 102,\n",
              " 'seem like': 103,\n",
              " 'self harm': 104,\n",
              " 'someon els': 105,\n",
              " 'someon talk': 106,\n",
              " 'someth like': 107,\n",
              " 'still feel': 108,\n",
              " 'suicid attempt': 109,\n",
              " 'take care': 110,\n",
              " 'take life': 111,\n",
              " 'talk peopl': 112,\n",
              " 'th grade': 113,\n",
              " 'thank read': 114,\n",
              " 'think get': 115,\n",
              " 'think go': 116,\n",
              " 'think kill': 117,\n",
              " 'think suicid': 118,\n",
              " 'think would': 119,\n",
              " 'time get': 120,\n",
              " 'time go': 121,\n",
              " 'treat like': 122,\n",
              " 'tri get': 123,\n",
              " 'tri hard': 124,\n",
              " 'tri help': 125,\n",
              " 'tri kill': 126,\n",
              " 'tri make': 127,\n",
              " 'ur ur': 128,\n",
              " 'video game': 129,\n",
              " 'want die': 130,\n",
              " 'want end': 131,\n",
              " 'want feel': 132,\n",
              " 'want get': 133,\n",
              " 'want go': 134,\n",
              " 'want help': 135,\n",
              " 'want hurt': 136,\n",
              " 'want kill': 137,\n",
              " 'want know': 138,\n",
              " 'want live': 139,\n",
              " 'want make': 140,\n",
              " 'want see': 141,\n",
              " 'want someon': 142,\n",
              " 'want stop': 143,\n",
              " 'want talk': 144,\n",
              " 'want want': 145,\n",
              " 'whole life': 146,\n",
              " 'wish could': 147,\n",
              " 'would like': 148,\n",
              " 'year old': 149}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "two_gram_vectorizer = CountVectorizer(ngram_range=(2,2), max_features=150)\n",
        "two_gram_training_count = two_gram_vectorizer.fit_transform(x_train)\n",
        "print(\"List of extracted tokens\")\n",
        "print(two_gram_vectorizer.get_feature_names_out())\n",
        "two_gram_vectorizer.vocabulary_"
      ],
      "id": "Hl7rGA6PPH3K"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Description of the word occurences data structure:\")\n",
        "print(type(two_gram_training_count))\n",
        "print(\"(Documents, Tokens)\")\n",
        "print(two_gram_training_count.shape)\n",
        "print(\"Word occurrences of the first document:\")\n",
        "print(two_gram_training_count[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-HIB5-HlmGL",
        "outputId": "e791fc21-11b2-4543-ad96-69f187cbc6fa"
      },
      "id": "y-HIB5-HlmGL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description of the word occurences data structure:\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "(Documents, Tokens)\n",
            "(92829, 150)\n",
            "Word occurrences of the first document:\n",
            "  (0, 112)\t2\n",
            "  (0, 19)\t1\n",
            "  (0, 94)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAuA8YhZRpCT"
      },
      "source": [
        "# Algorithms Implementation"
      ],
      "id": "GAuA8YhZRpCT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing all training texts and labels into a variable\n",
        "df_samples_text = df_samples['text']\n",
        "df_samples_label = df_samples['label']"
      ],
      "metadata": {
        "id": "U5-djTZfKuVG"
      },
      "id": "U5-djTZfKuVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEzbJ0QBRsTM"
      },
      "source": [
        "## General Function"
      ],
      "id": "UEzbJ0QBRsTM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "006e7e2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "\n",
        "kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=10)\n",
        "\n",
        "transformer = TfidfTransformer()\n",
        "one_gram_vectorizer = CountVectorizer(ngram_range=(1,1), max_features=150)\n",
        "two_gram_vectorizer = CountVectorizer(ngram_range=(2,2), max_features=150)\n",
        "\n",
        "def measure_accuracy(algorithm, vectorizer, dense=True):\n",
        "  list_time_train = []\n",
        "  list_time_predict = []\n",
        "  list_accuracy_score = []\n",
        "  list_precision_score = []\n",
        "  list_recall_score = []\n",
        "  list_f_measure_score = []\n",
        "  i = 1\n",
        "  for train_index, test_index in kfold.split(df_samples_text, df_samples_label):\n",
        "    x_train, x_test =  df_samples_text.iloc[train_index], df_samples_text.iloc[test_index]\n",
        "    y_train, y_test = df_samples_label.iloc[train_index], df_samples_label.iloc[test_index]\n",
        "    model_training_count = vectorizer.fit_transform(x_train)\n",
        "    model_new_count = vectorizer.transform(x_test)\n",
        "    if dense:\n",
        "      model_training_tfidf = transformer.fit_transform(model_training_count).todense()\n",
        "    else:\n",
        "      model_training_tfidf = transformer.fit_transform(model_training_count)\n",
        "\n",
        "    t0 = time.time()\n",
        "    model = algorithm.fit(model_training_tfidf, y_train)\n",
        "    train_time = round(time.time()-t0, 3)\n",
        "    list_time_train.append(train_time)\n",
        "    print(f'Predict time of fold {i}: {train_time}s')\n",
        "\n",
        "    model_tfidf = transformer.transform(model_new_count)\n",
        "\n",
        "    t1 = time.time()\n",
        "    model_prediction = model.predict(model_tfidf)\n",
        "    predict_time = round(time.time()-t0, 3)\n",
        "    list_time_predict.append(predict_time)\n",
        "    print(f'Predict time of fold {i}: {predict_time}s')\n",
        "\n",
        "    current_accuracy_score = accuracy_score(y_test, model_prediction)\n",
        "    current_precision_score = precision_score(y_test, model_prediction)\n",
        "    current_recall_score = recall_score(y_test, model_prediction)\n",
        "    current_f_measure_score = f1_score(y_test, model_prediction)\n",
        "    list_accuracy_score.append(current_accuracy_score)\n",
        "    list_precision_score.append(current_precision_score)\n",
        "    list_recall_score.append(current_recall_score)\n",
        "    list_f_measure_score.append(current_f_measure_score)\n",
        "    print(f'Accuracy of fold {i}: {current_accuracy_score}')\n",
        "    print(f'Precision of fold {i}: {current_precision_score}')\n",
        "    print(f'Recall of fold {i}: {current_recall_score}')\n",
        "    print(f'F-measure of fold {i}: {current_f_measure_score}')\n",
        "    print(metrics.classification_report(y_test, model_prediction, target_names=['non_suicide','suicide']))\n",
        "    i += 1\n",
        "  average_train_time = sum(list_time_train)/len(list_time_train)\n",
        "  average_predict_time = sum(list_time_predict)/len(list_time_predict)\n",
        "  average_accuracy_score = sum(list_accuracy_score)/len(list_accuracy_score)\n",
        "  average_precision_score = sum(list_precision_score)/len(list_precision_score)\n",
        "  average_recall_score = sum(list_recall_score)/len(list_recall_score)\n",
        "  average_f_measure_score = sum(list_f_measure_score)/len(list_f_measure_score)\n",
        "  average_values = {'average_train_time':average_train_time,'average_predict_time':average_predict_time,\n",
        "                    'average_accuracy':average_accuracy_score,'average_precision':average_precision_score,\n",
        "                    'average_recall':average_recall_score,'average_f_measure':average_f_measure_score}\n",
        "  return average_values"
      ],
      "id": "006e7e2f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "DlA1s-YWVRv_"
      },
      "id": "DlA1s-YWVRv_"
    },
    {
      "cell_type": "code",
      "source": [
        "one_gram_mnb_average = measure_accuracy(MultinomialNB(), one_gram_vectorizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5iRlxPDL82-",
        "outputId": "982f4da5-faef-4bae-8e87-5327391c698a"
      },
      "id": "_5iRlxPDL82-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict time of fold 1: 0.064s\n",
            "Predict time of fold 1: 0.078s\n",
            "Accuracy of fold 1: 0.8045501551189245\n",
            "Precision of fold 1: 0.7753522217061465\n",
            "Recall of fold 1: 0.8597424892703862\n",
            "F-measure of fold 1: 0.8153695864539238\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5779\n",
            "     suicide       0.78      0.86      0.82      5825\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 2: 0.066s\n",
            "Predict time of fold 2: 0.081s\n",
            "Accuracy of fold 2: 0.8016201309893141\n",
            "Precision of fold 2: 0.769254562326013\n",
            "Recall of fold 2: 0.8599585062240664\n",
            "F-measure of fold 2: 0.8120816326530612\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.74      0.79      5820\n",
            "     suicide       0.77      0.86      0.81      5784\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 3: 0.068s\n",
            "Predict time of fold 3: 0.082s\n",
            "Accuracy of fold 3: 0.8076525336091003\n",
            "Precision of fold 3: 0.7830508474576271\n",
            "Recall of fold 3: 0.8604808669150017\n",
            "F-measure of fold 3: 0.8199419167473377\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5698\n",
            "     suicide       0.78      0.86      0.82      5906\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.81      0.81     11604\n",
            "weighted avg       0.81      0.81      0.81     11604\n",
            "\n",
            "Predict time of fold 4: 0.065s\n",
            "Predict time of fold 4: 0.081s\n",
            "Accuracy of fold 4: 0.8100654946570148\n",
            "Precision of fold 4: 0.7819467937874827\n",
            "Recall of fold 4: 0.8661216147164027\n",
            "F-measure of fold 4: 0.8218845967350898\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.75      0.80      5733\n",
            "     suicide       0.78      0.87      0.82      5871\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.81      0.81     11604\n",
            "weighted avg       0.81      0.81      0.81     11604\n",
            "\n",
            "Predict time of fold 5: 0.066s\n",
            "Predict time of fold 5: 0.082s\n",
            "Accuracy of fold 5: 0.8015339538090314\n",
            "Precision of fold 5: 0.767416934619507\n",
            "Recall of fold 5: 0.8647342995169082\n",
            "F-measure of fold 5: 0.8131743327654742\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.74      0.79      5808\n",
            "     suicide       0.77      0.86      0.81      5796\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 6: 0.066s\n",
            "Predict time of fold 6: 0.083s\n",
            "Accuracy of fold 6: 0.8015339538090314\n",
            "Precision of fold 6: 0.772705955916836\n",
            "Recall of fold 6: 0.8534185082872928\n",
            "F-measure of fold 6: 0.8110591516941504\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5812\n",
            "     suicide       0.77      0.85      0.81      5792\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.80      0.80      0.80     11604\n",
            "weighted avg       0.80      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 7: 0.066s\n",
            "Predict time of fold 7: 0.082s\n",
            "Accuracy of fold 7: 0.8051533953809031\n",
            "Precision of fold 7: 0.7804017788682718\n",
            "Recall of fold 7: 0.8599188915174045\n",
            "F-measure of fold 7: 0.8182329769274057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5686\n",
            "     suicide       0.78      0.86      0.82      5918\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.81      0.80     11604\n",
            "\n",
            "Predict time of fold 8: 0.066s\n",
            "Predict time of fold 8: 0.082s\n",
            "Accuracy of fold 8: 0.8074635870033612\n",
            "Precision of fold 8: 0.777306967984934\n",
            "Recall of fold 8: 0.8587031900138696\n",
            "F-measure of fold 8: 0.8159802306425041\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.76      0.80      5835\n",
            "     suicide       0.78      0.86      0.82      5768\n",
            "\n",
            "    accuracy                           0.81     11603\n",
            "   macro avg       0.81      0.81      0.81     11603\n",
            "weighted avg       0.81      0.81      0.81     11603\n",
            "\n",
            "Predict time of fold 9: 0.066s\n",
            "Predict time of fold 9: 0.08s\n",
            "Accuracy of fold 9: 0.8061708178919245\n",
            "Precision of fold 9: 0.7716645885286783\n",
            "Recall of fold 9: 0.8632955536181343\n",
            "F-measure of fold 9: 0.814912352892766\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.75      0.80      5868\n",
            "     suicide       0.77      0.86      0.81      5735\n",
            "\n",
            "    accuracy                           0.81     11603\n",
            "   macro avg       0.81      0.81      0.81     11603\n",
            "weighted avg       0.81      0.81      0.81     11603\n",
            "\n",
            "Predict time of fold 10: 0.068s\n",
            "Predict time of fold 10: 0.083s\n",
            "Accuracy of fold 10: 0.8041023873136258\n",
            "Precision of fold 10: 0.7736554949337491\n",
            "Recall of fold 10: 0.8580567081604425\n",
            "F-measure of fold 10: 0.8136732519058939\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5819\n",
            "     suicide       0.77      0.86      0.81      5784\n",
            "\n",
            "    accuracy                           0.80     11603\n",
            "   macro avg       0.81      0.80      0.80     11603\n",
            "weighted avg       0.81      0.80      0.80     11603\n",
            "\n",
            "Predict time of fold 11: 0.065s\n",
            "Predict time of fold 11: 0.08s\n",
            "Accuracy of fold 11: 0.8089451913133402\n",
            "Precision of fold 11: 0.7795398009950248\n",
            "Recall of fold 11: 0.8625494581111303\n",
            "F-measure of fold 11: 0.8189465087790936\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.76      0.80      5791\n",
            "     suicide       0.78      0.86      0.82      5813\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.81      0.81     11604\n",
            "weighted avg       0.81      0.81      0.81     11604\n",
            "\n",
            "Predict time of fold 12: 0.064s\n",
            "Predict time of fold 12: 0.082s\n",
            "Accuracy of fold 12: 0.8032574974146846\n",
            "Precision of fold 12: 0.7704995287464655\n",
            "Recall of fold 12: 0.8564693556836039\n",
            "F-measure of fold 12: 0.811213098486728\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5877\n",
            "     suicide       0.77      0.86      0.81      5727\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 13: 0.066s\n",
            "Predict time of fold 13: 0.081s\n",
            "Accuracy of fold 13: 0.8095484315753189\n",
            "Precision of fold 13: 0.7745718253355963\n",
            "Recall of fold 13: 0.8701681400589357\n",
            "F-measure of fold 13: 0.8195918367346938\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.75      0.80      5835\n",
            "     suicide       0.77      0.87      0.82      5769\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.81      0.81     11604\n",
            "weighted avg       0.81      0.81      0.81     11604\n",
            "\n",
            "Predict time of fold 14: 0.068s\n",
            "Predict time of fold 14: 0.083s\n",
            "Accuracy of fold 14: 0.8012754222681834\n",
            "Precision of fold 14: 0.7694680030840401\n",
            "Recall of fold 14: 0.8601965178417514\n",
            "F-measure of fold 14: 0.8123066905420804\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.74      0.79      5803\n",
            "     suicide       0.77      0.86      0.81      5801\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 15: 0.065s\n",
            "Predict time of fold 15: 0.081s\n",
            "Accuracy of fold 15: 0.7999827645639435\n",
            "Precision of fold 15: 0.7718944099378882\n",
            "Recall of fold 15: 0.8536836682122617\n",
            "F-measure of fold 15: 0.810731468645519\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5781\n",
            "     suicide       0.77      0.85      0.81      5823\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.80      0.80      0.80     11604\n",
            "weighted avg       0.80      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 16: 0.066s\n",
            "Predict time of fold 16: 0.081s\n",
            "Accuracy of fold 16: 0.8042054463977939\n",
            "Precision of fold 16: 0.7744714173844949\n",
            "Recall of fold 16: 0.8559806127747966\n",
            "F-measure of fold 16: 0.813188620292715\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5827\n",
            "     suicide       0.77      0.86      0.81      5777\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 17: 0.064s\n",
            "Predict time of fold 17: 0.08s\n",
            "Accuracy of fold 17: 0.8080834195105137\n",
            "Precision of fold 17: 0.7778282598818719\n",
            "Recall of fold 17: 0.8710990502035278\n",
            "F-measure of fold 17: 0.8218257460596848\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.74      0.79      5708\n",
            "     suicide       0.78      0.87      0.82      5896\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.81      0.81     11604\n",
            "weighted avg       0.81      0.81      0.81     11604\n",
            "\n",
            "Predict time of fold 18: 0.064s\n",
            "Predict time of fold 18: 0.079s\n",
            "Accuracy of fold 18: 0.8124622942342498\n",
            "Precision of fold 18: 0.7884674683934628\n",
            "Recall of fold 18: 0.8641432916525853\n",
            "F-measure of fold 18: 0.8245727184779104\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.76      0.80      5685\n",
            "     suicide       0.79      0.86      0.82      5918\n",
            "\n",
            "    accuracy                           0.81     11603\n",
            "   macro avg       0.82      0.81      0.81     11603\n",
            "weighted avg       0.82      0.81      0.81     11603\n",
            "\n",
            "Predict time of fold 19: 0.067s\n",
            "Predict time of fold 19: 0.082s\n",
            "Accuracy of fold 19: 0.7962595880375765\n",
            "Precision of fold 19: 0.766417678182384\n",
            "Recall of fold 19: 0.850898410504492\n",
            "F-measure of fold 19: 0.8064516129032258\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.74      0.78      5815\n",
            "     suicide       0.77      0.85      0.81      5788\n",
            "\n",
            "    accuracy                           0.80     11603\n",
            "   macro avg       0.80      0.80      0.80     11603\n",
            "weighted avg       0.80      0.80      0.80     11603\n",
            "\n",
            "Predict time of fold 20: 0.064s\n",
            "Predict time of fold 20: 0.08s\n",
            "Accuracy of fold 20: 0.8078083254330777\n",
            "Precision of fold 20: 0.7810230258074486\n",
            "Recall of fold 20: 0.8614283279359127\n",
            "F-measure of fold 20: 0.819257578213649\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5736\n",
            "     suicide       0.78      0.86      0.82      5867\n",
            "\n",
            "    accuracy                           0.81     11603\n",
            "   macro avg       0.81      0.81      0.81     11603\n",
            "weighted avg       0.81      0.81      0.81     11603\n",
            "\n",
            "Predict time of fold 21: 0.067s\n",
            "Predict time of fold 21: 0.083s\n",
            "Accuracy of fold 21: 0.8046363322992072\n",
            "Precision of fold 21: 0.7720450281425891\n",
            "Recall of fold 21: 0.8592309030798677\n",
            "F-measure of fold 21: 0.8133080787284854\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.80      5857\n",
            "     suicide       0.77      0.86      0.81      5747\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.81      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 22: 0.071s\n",
            "Predict time of fold 22: 0.087s\n",
            "Accuracy of fold 22: 0.7998965873836608\n",
            "Precision of fold 22: 0.7692188708430008\n",
            "Recall of fold 22: 0.8569705324831983\n",
            "F-measure of fold 22: 0.8107270948809912\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.74      0.79      5801\n",
            "     suicide       0.77      0.86      0.81      5803\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.80      0.80      0.80     11604\n",
            "weighted avg       0.80      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 23: 0.065s\n",
            "Predict time of fold 23: 0.081s\n",
            "Accuracy of fold 23: 0.8050672182006204\n",
            "Precision of fold 23: 0.7791516669275317\n",
            "Recall of fold 23: 0.8540058329044433\n",
            "F-measure of fold 23: 0.814863316418399\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.76      0.79      5775\n",
            "     suicide       0.78      0.85      0.81      5829\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.81      0.80     11604\n",
            "\n",
            "Predict time of fold 24: 0.069s\n",
            "Predict time of fold 24: 0.085s\n",
            "Accuracy of fold 24: 0.8033436745949672\n",
            "Precision of fold 24: 0.7730278333077041\n",
            "Recall of fold 24: 0.8618206754671696\n",
            "F-measure of fold 24: 0.8150129701686122\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.74      0.79      5771\n",
            "     suicide       0.77      0.86      0.82      5833\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 25: 0.068s\n",
            "Predict time of fold 25: 0.083s\n",
            "Accuracy of fold 25: 0.806963116166839\n",
            "Precision of fold 25: 0.7828384890408829\n",
            "Recall of fold 25: 0.8566082667120258\n",
            "F-measure of fold 25: 0.8180636777128005\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.76      0.79      5725\n",
            "     suicide       0.78      0.86      0.82      5879\n",
            "\n",
            "    accuracy                           0.81     11604\n",
            "   macro avg       0.81      0.81      0.81     11604\n",
            "weighted avg       0.81      0.81      0.81     11604\n",
            "\n",
            "Predict time of fold 26: 0.065s\n",
            "Predict time of fold 26: 0.08s\n",
            "Accuracy of fold 26: 0.80342985177525\n",
            "Precision of fold 26: 0.7719352351580571\n",
            "Recall of fold 26: 0.8619146005509641\n",
            "F-measure of fold 26: 0.8144472464003905\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.74      0.79      5796\n",
            "     suicide       0.77      0.86      0.81      5808\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 27: 0.068s\n",
            "Predict time of fold 27: 0.084s\n",
            "Accuracy of fold 27: 0.8047225094794899\n",
            "Precision of fold 27: 0.7763320941759604\n",
            "Recall of fold 27: 0.8591018169352074\n",
            "F-measure of fold 27: 0.8156224572823434\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5770\n",
            "     suicide       0.78      0.86      0.82      5834\n",
            "\n",
            "    accuracy                           0.80     11604\n",
            "   macro avg       0.81      0.80      0.80     11604\n",
            "weighted avg       0.81      0.80      0.80     11604\n",
            "\n",
            "Predict time of fold 28: 0.066s\n",
            "Predict time of fold 28: 0.082s\n",
            "Accuracy of fold 28: 0.8140136171679738\n",
            "Precision of fold 28: 0.7857252967473408\n",
            "Recall of fold 28: 0.8690537084398977\n",
            "F-measure of fold 28: 0.825291450777202\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.76      0.80      5738\n",
            "     suicide       0.79      0.87      0.83      5865\n",
            "\n",
            "    accuracy                           0.81     11603\n",
            "   macro avg       0.82      0.81      0.81     11603\n",
            "weighted avg       0.82      0.81      0.81     11603\n",
            "\n",
            "Predict time of fold 29: 0.065s\n",
            "Predict time of fold 29: 0.082s\n",
            "Accuracy of fold 29: 0.8039300180987675\n",
            "Precision of fold 29: 0.7645437844458053\n",
            "Recall of fold 29: 0.8714011516314779\n",
            "F-measure of fold 29: 0.8144825899045911\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.85      0.74      0.79      5872\n",
            "     suicide       0.76      0.87      0.81      5731\n",
            "\n",
            "    accuracy                           0.80     11603\n",
            "   macro avg       0.81      0.80      0.80     11603\n",
            "weighted avg       0.81      0.80      0.80     11603\n",
            "\n",
            "Predict time of fold 30: 0.066s\n",
            "Predict time of fold 30: 0.081s\n",
            "Accuracy of fold 30: 0.8062570024993536\n",
            "Precision of fold 30: 0.7803549190535491\n",
            "Recall of fold 30: 0.8569230769230769\n",
            "F-measure of fold 30: 0.8168486231057519\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.84      0.75      0.79      5753\n",
            "     suicide       0.78      0.86      0.82      5850\n",
            "\n",
            "    accuracy                           0.81     11603\n",
            "   macro avg       0.81      0.81      0.81     11603\n",
            "weighted avg       0.81      0.81      0.81     11603\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_gram_mnb_average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cotaqHSqIibS",
        "outputId": "708a6de8-d1db-4140-9d55-f9179c947c01"
      },
      "id": "cotaqHSqIibS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_accuracy': 0.8051311572665678,\n",
              " 'average_f_measure': 0.8157687804644159,\n",
              " 'average_precision': 0.7754036927240117,\n",
              " 'average_predict_time': 0.08169999999999998,\n",
              " 'average_recall': 0.860602600878208,\n",
              " 'average_train_time': 0.06613333333333334}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_gram_mnb_average = measure_accuracy(MultinomialNB(), two_gram_vectorizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpc0IdSnL9eN",
        "outputId": "5dcfc1c1-f3e3-4e30-ffce-984543bfae8b"
      },
      "id": "Gpc0IdSnL9eN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict time of fold 1: 0.083s\n",
            "Predict time of fold 1: 0.092s\n",
            "Accuracy of fold 1: 0.6527059634608756\n",
            "Precision of fold 1: 0.6179524247601524\n",
            "Recall of fold 1: 0.8072103004291845\n",
            "F-measure of fold 1: 0.7000148875986304\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.50      0.59      5779\n",
            "     suicide       0.62      0.81      0.70      5825\n",
            "\n",
            "    accuracy                           0.65     11604\n",
            "   macro avg       0.67      0.65      0.64     11604\n",
            "weighted avg       0.67      0.65      0.64     11604\n",
            "\n",
            "Predict time of fold 2: 0.064s\n",
            "Predict time of fold 2: 0.071s\n",
            "Accuracy of fold 2: 0.6539124439848328\n",
            "Precision of fold 2: 0.6172413793103448\n",
            "Recall of fold 2: 0.8046334716459198\n",
            "F-measure of fold 2: 0.698589012308616\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.50      0.59      5820\n",
            "     suicide       0.62      0.80      0.70      5784\n",
            "\n",
            "    accuracy                           0.65     11604\n",
            "   macro avg       0.67      0.65      0.65     11604\n",
            "weighted avg       0.67      0.65      0.65     11604\n",
            "\n",
            "Predict time of fold 3: 0.065s\n",
            "Predict time of fold 3: 0.072s\n",
            "Accuracy of fold 3: 0.6692519820751465\n",
            "Precision of fold 3: 0.6372080679405521\n",
            "Recall of fold 3: 0.8130714527599052\n",
            "F-measure of fold 3: 0.7144770123493528\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.52      0.61      5698\n",
            "     suicide       0.64      0.81      0.71      5906\n",
            "\n",
            "    accuracy                           0.67     11604\n",
            "   macro avg       0.68      0.67      0.66     11604\n",
            "weighted avg       0.68      0.67      0.66     11604\n",
            "\n",
            "Predict time of fold 4: 0.069s\n",
            "Predict time of fold 4: 0.076s\n",
            "Accuracy of fold 4: 0.6594277835229231\n",
            "Precision of fold 4: 0.6266332321499274\n",
            "Recall of fold 4: 0.8087208312042241\n",
            "F-measure of fold 4: 0.7061273051754907\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5733\n",
            "     suicide       0.63      0.81      0.71      5871\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 5: 0.066s\n",
            "Predict time of fold 5: 0.073s\n",
            "Accuracy of fold 5: 0.6571009996552912\n",
            "Precision of fold 5: 0.6209237321975243\n",
            "Recall of fold 5: 0.8048654244306418\n",
            "F-measure of fold 5: 0.7010293786159741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5808\n",
            "     suicide       0.62      0.80      0.70      5796\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 6: 0.067s\n",
            "Predict time of fold 6: 0.074s\n",
            "Accuracy of fold 6: 0.6620130989314029\n",
            "Precision of fold 6: 0.6241699867197875\n",
            "Recall of fold 6: 0.81146408839779\n",
            "F-measure of fold 6: 0.7055997597958265\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.51      0.60      5812\n",
            "     suicide       0.62      0.81      0.71      5792\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.68      0.66      0.65     11604\n",
            "weighted avg       0.68      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 7: 0.066s\n",
            "Predict time of fold 7: 0.073s\n",
            "Accuracy of fold 7: 0.6617545673905549\n",
            "Precision of fold 7: 0.634498582804697\n",
            "Recall of fold 7: 0.7943562014193984\n",
            "F-measure of fold 7: 0.7054851054250769\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.71      0.52      0.60      5686\n",
            "     suicide       0.63      0.79      0.71      5918\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.66     11604\n",
            "\n",
            "Predict time of fold 8: 0.069s\n",
            "Predict time of fold 8: 0.076s\n",
            "Accuracy of fold 8: 0.663362923381884\n",
            "Precision of fold 8: 0.625336564351104\n",
            "Recall of fold 8: 0.8053051317614425\n",
            "F-measure of fold 8: 0.704001212488633\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.52      0.61      5835\n",
            "     suicide       0.63      0.81      0.70      5768\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.68      0.66      0.66     11603\n",
            "weighted avg       0.68      0.66      0.66     11603\n",
            "\n",
            "Predict time of fold 9: 0.068s\n",
            "Predict time of fold 9: 0.075s\n",
            "Accuracy of fold 9: 0.658795139188141\n",
            "Precision of fold 9: 0.6189073379753616\n",
            "Recall of fold 9: 0.8059285091543156\n",
            "F-measure of fold 9: 0.700143906687874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.51      0.60      5868\n",
            "     suicide       0.62      0.81      0.70      5735\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.67      0.66      0.65     11603\n",
            "weighted avg       0.68      0.66      0.65     11603\n",
            "\n",
            "Predict time of fold 10: 0.067s\n",
            "Predict time of fold 10: 0.074s\n",
            "Accuracy of fold 10: 0.6614668620184435\n",
            "Precision of fold 10: 0.6245972073039742\n",
            "Recall of fold 10: 0.8042876901798064\n",
            "F-measure of fold 10: 0.7031438935912938\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.52      0.61      5819\n",
            "     suicide       0.62      0.80      0.70      5784\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.68      0.66      0.65     11603\n",
            "weighted avg       0.68      0.66      0.65     11603\n",
            "\n",
            "Predict time of fold 11: 0.069s\n",
            "Predict time of fold 11: 0.077s\n",
            "Accuracy of fold 11: 0.6592554291623578\n",
            "Precision of fold 11: 0.6236201622556191\n",
            "Recall of fold 11: 0.8066402890073973\n",
            "F-measure of fold 11: 0.7034203420342034\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5791\n",
            "     suicide       0.62      0.81      0.70      5813\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 12: 0.067s\n",
            "Predict time of fold 12: 0.075s\n",
            "Accuracy of fold 12: 0.6608066184074457\n",
            "Precision of fold 12: 0.6197032482288464\n",
            "Recall of fold 12: 0.8094988650253186\n",
            "F-measure of fold 12: 0.7019987886129617\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.74      0.52      0.61      5877\n",
            "     suicide       0.62      0.81      0.70      5727\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.68      0.66      0.65     11604\n",
            "weighted avg       0.68      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 13: 0.079s\n",
            "Predict time of fold 13: 0.088s\n",
            "Accuracy of fold 13: 0.6645984143398828\n",
            "Precision of fold 13: 0.6252836737418235\n",
            "Recall of fold 13: 0.811925810365748\n",
            "F-measure of fold 13: 0.7064856711915535\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.74      0.52      0.61      5835\n",
            "     suicide       0.63      0.81      0.71      5769\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.68      0.67      0.66     11604\n",
            "weighted avg       0.68      0.66      0.66     11604\n",
            "\n",
            "Predict time of fold 14: 0.065s\n",
            "Predict time of fold 14: 0.073s\n",
            "Accuracy of fold 14: 0.6524474319200276\n",
            "Precision of fold 14: 0.6174282678002125\n",
            "Recall of fold 14: 0.8012411653163247\n",
            "F-measure of fold 14: 0.6974266636656913\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.50      0.59      5803\n",
            "     suicide       0.62      0.80      0.70      5801\n",
            "\n",
            "    accuracy                           0.65     11604\n",
            "   macro avg       0.67      0.65      0.64     11604\n",
            "weighted avg       0.67      0.65      0.64     11604\n",
            "\n",
            "Predict time of fold 15: 0.074s\n",
            "Predict time of fold 15: 0.081s\n",
            "Accuracy of fold 15: 0.6641675284384695\n",
            "Precision of fold 15: 0.6291577253218884\n",
            "Recall of fold 15: 0.8055984887515026\n",
            "F-measure of fold 15: 0.7065291061073876\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.52      0.61      5781\n",
            "     suicide       0.63      0.81      0.71      5823\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.68      0.66      0.66     11604\n",
            "weighted avg       0.68      0.66      0.66     11604\n",
            "\n",
            "Predict time of fold 16: 0.076s\n",
            "Predict time of fold 16: 0.083s\n",
            "Accuracy of fold 16: 0.6558083419510513\n",
            "Precision of fold 16: 0.6184402816527169\n",
            "Recall of fold 16: 0.8057815475160117\n",
            "F-measure of fold 16: 0.699789536981359\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5827\n",
            "     suicide       0.62      0.81      0.70      5777\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 17: 0.068s\n",
            "Predict time of fold 17: 0.075s\n",
            "Accuracy of fold 17: 0.6608927955877284\n",
            "Precision of fold 17: 0.631240797751305\n",
            "Recall of fold 17: 0.7998643147896879\n",
            "F-measure of fold 17: 0.7056183137577616\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.71      0.52      0.60      5708\n",
            "     suicide       0.63      0.80      0.71      5896\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 18: 0.07s\n",
            "Predict time of fold 18: 0.077s\n",
            "Accuracy of fold 18: 0.667155046108765\n",
            "Precision of fold 18: 0.6351209253417456\n",
            "Recall of fold 18: 0.8164920581277458\n",
            "F-measure of fold 18: 0.7144758243383114\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.51      0.60      5685\n",
            "     suicide       0.64      0.82      0.71      5918\n",
            "\n",
            "    accuracy                           0.67     11603\n",
            "   macro avg       0.68      0.66      0.66     11603\n",
            "weighted avg       0.68      0.67      0.66     11603\n",
            "\n",
            "Predict time of fold 19: 0.069s\n",
            "Predict time of fold 19: 0.077s\n",
            "Accuracy of fold 19: 0.6557786779281221\n",
            "Precision of fold 19: 0.619408945686901\n",
            "Recall of fold 19: 0.8039046302695232\n",
            "F-measure of fold 19: 0.6996992481203008\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5815\n",
            "     suicide       0.62      0.80      0.70      5788\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.67      0.66      0.65     11603\n",
            "weighted avg       0.67      0.66      0.65     11603\n",
            "\n",
            "Predict time of fold 20: 0.068s\n",
            "Predict time of fold 20: 0.075s\n",
            "Accuracy of fold 20: 0.6593984314401448\n",
            "Precision of fold 20: 0.6286789410025534\n",
            "Recall of fold 20: 0.797341060167036\n",
            "F-measure of fold 20: 0.7030357679591223\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.71      0.52      0.60      5736\n",
            "     suicide       0.63      0.80      0.70      5867\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.67      0.66      0.65     11603\n",
            "weighted avg       0.67      0.66      0.65     11603\n",
            "\n",
            "Predict time of fold 21: 0.066s\n",
            "Predict time of fold 21: 0.073s\n",
            "Accuracy of fold 21: 0.6552051016890728\n",
            "Precision of fold 21: 0.6184531886024424\n",
            "Recall of fold 21: 0.793109448407865\n",
            "F-measure of fold 21: 0.6949759853625067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.52      0.60      5857\n",
            "     suicide       0.62      0.79      0.69      5747\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 22: 0.067s\n",
            "Predict time of fold 22: 0.074s\n",
            "Accuracy of fold 22: 0.651844191658049\n",
            "Precision of fold 22: 0.6170806215964936\n",
            "Recall of fold 22: 0.8006203687747717\n",
            "F-measure of fold 22: 0.696969696969697\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.50      0.59      5801\n",
            "     suicide       0.62      0.80      0.70      5803\n",
            "\n",
            "    accuracy                           0.65     11604\n",
            "   macro avg       0.67      0.65      0.64     11604\n",
            "weighted avg       0.67      0.65      0.64     11604\n",
            "\n",
            "Predict time of fold 23: 0.068s\n",
            "Predict time of fold 23: 0.075s\n",
            "Accuracy of fold 23: 0.6601172009651844\n",
            "Precision of fold 23: 0.6256834244565942\n",
            "Recall of fold 23: 0.8049408131755018\n",
            "F-measure of fold 23: 0.7040816326530611\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5775\n",
            "     suicide       0.63      0.80      0.70      5829\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 24: 0.068s\n",
            "Predict time of fold 24: 0.075s\n",
            "Accuracy of fold 24: 0.6583074801792486\n",
            "Precision of fold 24: 0.6236431029917924\n",
            "Recall of fold 24: 0.8076461512086405\n",
            "F-measure of fold 24: 0.7038171360274894\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5771\n",
            "     suicide       0.62      0.81      0.70      5833\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.67      0.66      0.65     11604\n",
            "weighted avg       0.67      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 25: 0.066s\n",
            "Predict time of fold 25: 0.073s\n",
            "Accuracy of fold 25: 0.6636504653567735\n",
            "Precision of fold 25: 0.6325818572195384\n",
            "Recall of fold 25: 0.8018370471168567\n",
            "F-measure of fold 25: 0.7072237641587279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.52      0.60      5725\n",
            "     suicide       0.63      0.80      0.71      5879\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.68      0.66      0.66     11604\n",
            "weighted avg       0.68      0.66      0.66     11604\n",
            "\n",
            "Predict time of fold 26: 0.07s\n",
            "Predict time of fold 26: 0.077s\n",
            "Accuracy of fold 26: 0.6615822130299897\n",
            "Precision of fold 26: 0.625684885741013\n",
            "Recall of fold 26: 0.806129476584022\n",
            "F-measure of fold 26: 0.7045369046723347\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.52      0.60      5796\n",
            "     suicide       0.63      0.81      0.70      5808\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.68      0.66      0.65     11604\n",
            "weighted avg       0.68      0.66      0.65     11604\n",
            "\n",
            "Predict time of fold 27: 0.067s\n",
            "Predict time of fold 27: 0.075s\n",
            "Accuracy of fold 27: 0.6639951740779042\n",
            "Precision of fold 27: 0.6293968169051759\n",
            "Recall of fold 27: 0.8066506684950291\n",
            "F-measure of fold 27: 0.7070843663135753\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.52      0.61      5770\n",
            "     suicide       0.63      0.81      0.71      5834\n",
            "\n",
            "    accuracy                           0.66     11604\n",
            "   macro avg       0.68      0.66      0.66     11604\n",
            "weighted avg       0.68      0.66      0.66     11604\n",
            "\n",
            "Predict time of fold 28: 0.069s\n",
            "Predict time of fold 28: 0.075s\n",
            "Accuracy of fold 28: 0.6643971386710333\n",
            "Precision of fold 28: 0.6309634551495017\n",
            "Recall of fold 28: 0.8095481670929241\n",
            "F-measure of fold 28: 0.7091859596713965\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.52      0.60      5738\n",
            "     suicide       0.63      0.81      0.71      5865\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.68      0.66      0.66     11603\n",
            "weighted avg       0.68      0.66      0.66     11603\n",
            "\n",
            "Predict time of fold 29: 0.066s\n",
            "Predict time of fold 29: 0.073s\n",
            "Accuracy of fold 29: 0.6585365853658537\n",
            "Precision of fold 29: 0.6173855341738553\n",
            "Recall of fold 29: 0.811725702320712\n",
            "F-measure of fold 29: 0.7013417759686417\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.73      0.51      0.60      5872\n",
            "     suicide       0.62      0.81      0.70      5731\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.68      0.66      0.65     11603\n",
            "weighted avg       0.68      0.66      0.65     11603\n",
            "\n",
            "Predict time of fold 30: 0.067s\n",
            "Predict time of fold 30: 0.075s\n",
            "Accuracy of fold 30: 0.6593122468327157\n",
            "Precision of fold 30: 0.6254131958217638\n",
            "Recall of fold 30: 0.8085470085470086\n",
            "F-measure of fold 30: 0.705285916648028\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.72      0.51      0.60      5753\n",
            "     suicide       0.63      0.81      0.71      5850\n",
            "\n",
            "    accuracy                           0.66     11603\n",
            "   macro avg       0.67      0.66      0.65     11603\n",
            "weighted avg       0.67      0.66      0.65     11603\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_gram_mnb_average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOx-bb7yLdGt",
        "outputId": "af6c3354-5643-4b1d-a33d-04771574c476"
      },
      "id": "vOx-bb7yLdGt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_accuracy': 0.6599016092239772,\n",
              " 'average_f_measure': 0.7037197958416961,\n",
              " 'average_precision': 0.624727918898507,\n",
              " 'average_predict_time': 0.07606666666666666,\n",
              " 'average_recall': 0.8056295394147419,\n",
              " 'average_train_time': 0.06876666666666667}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM"
      ],
      "metadata": {
        "id": "xbZRrlEqptv6"
      },
      "id": "xbZRrlEqptv6"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "linearSVC = CalibratedClassifierCV(svm.LinearSVC())"
      ],
      "metadata": {
        "id": "4pHsGGeDW-5d"
      },
      "id": "4pHsGGeDW-5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_gram_svc_average = measure_accuracy(linearSVC, one_gram_vectorizer, dense=False)\n",
        "print(f'1-Gram Linear Support Vector Classification Average: {one_gram_svc_average}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnHAr8ZMp_yo",
        "outputId": "bbac1fa5-acea-46dd-bc91-6e5ac276ff6e"
      },
      "id": "OnHAr8ZMp_yo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict time of fold 1: 7.295s\n",
            "Predict time of fold 1: 7.318s\n",
            "Accuracy of fold 1: 0.8818510858324715\n",
            "Precision of fold 1: 0.8955595026642984\n",
            "Recall of fold 1: 0.865579399141631\n",
            "F-measure of fold 1: 0.8803142732431254\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5779\n",
            "     suicide       0.90      0.87      0.88      5825\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 2: 6.942s\n",
            "Predict time of fold 2: 6.964s\n",
            "Accuracy of fold 2: 0.8751292657704239\n",
            "Precision of fold 2: 0.8882321332616873\n",
            "Recall of fold 2: 0.8573651452282157\n",
            "F-measure of fold 2: 0.8725257323832145\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5820\n",
            "     suicide       0.89      0.86      0.87      5784\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 3: 7.104s\n",
            "Predict time of fold 3: 7.126s\n",
            "Accuracy of fold 3: 0.8815925542916235\n",
            "Precision of fold 3: 0.8978230337078652\n",
            "Recall of fold 3: 0.8658990856755842\n",
            "F-measure of fold 3: 0.8815721427340114\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5698\n",
            "     suicide       0.90      0.87      0.88      5906\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 4: 6.78s\n",
            "Predict time of fold 4: 6.802s\n",
            "Accuracy of fold 4: 0.8816787314719062\n",
            "Precision of fold 4: 0.8934569629111266\n",
            "Recall of fold 4: 0.8698688468744677\n",
            "F-measure of fold 4: 0.8815051350651592\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5733\n",
            "     suicide       0.89      0.87      0.88      5871\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 5: 6.828s\n",
            "Predict time of fold 5: 6.859s\n",
            "Accuracy of fold 5: 0.880730782488797\n",
            "Precision of fold 5: 0.89002828854314\n",
            "Recall of fold 5: 0.8685300207039337\n",
            "F-measure of fold 5: 0.8791477471184073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5808\n",
            "     suicide       0.89      0.87      0.88      5796\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 6: 6.901s\n",
            "Predict time of fold 6: 6.923s\n",
            "Accuracy of fold 6: 0.8778007583591865\n",
            "Precision of fold 6: 0.8910944206008584\n",
            "Recall of fold 6: 0.8603245856353591\n",
            "F-measure of fold 6: 0.8754392129304286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5812\n",
            "     suicide       0.89      0.86      0.88      5792\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 7: 6.745s\n",
            "Predict time of fold 7: 6.766s\n",
            "Accuracy of fold 7: 0.8761633919338159\n",
            "Precision of fold 7: 0.89286340522532\n",
            "Recall of fold 7: 0.8604258195336262\n",
            "F-measure of fold 7: 0.8763445486619051\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5686\n",
            "     suicide       0.89      0.86      0.88      5918\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 8: 6.864s\n",
            "Predict time of fold 8: 6.886s\n",
            "Accuracy of fold 8: 0.8769283805912264\n",
            "Precision of fold 8: 0.8875\n",
            "Recall of fold 8: 0.8616504854368932\n",
            "F-measure of fold 8: 0.874384236453202\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5835\n",
            "     suicide       0.89      0.86      0.87      5768\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 9: 6.992s\n",
            "Predict time of fold 9: 7.014s\n",
            "Accuracy of fold 9: 0.8783935189175214\n",
            "Precision of fold 9: 0.8859335951445912\n",
            "Recall of fold 9: 0.8653879686137751\n",
            "F-measure of fold 9: 0.875540266384405\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5868\n",
            "     suicide       0.89      0.87      0.88      5735\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 10: 6.811s\n",
            "Predict time of fold 10: 6.833s\n",
            "Accuracy of fold 10: 0.8736533655089201\n",
            "Precision of fold 10: 0.8859492313192706\n",
            "Recall of fold 10: 0.8568464730290456\n",
            "F-measure of fold 10: 0.8711548602566355\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5819\n",
            "     suicide       0.89      0.86      0.87      5784\n",
            "\n",
            "    accuracy                           0.87     11603\n",
            "   macro avg       0.87      0.87      0.87     11603\n",
            "weighted avg       0.87      0.87      0.87     11603\n",
            "\n",
            "Predict time of fold 11: 6.773s\n",
            "Predict time of fold 11: 6.794s\n",
            "Accuracy of fold 11: 0.8827990348155809\n",
            "Precision of fold 11: 0.8958222222222222\n",
            "Recall of fold 11: 0.866850163426802\n",
            "F-measure of fold 11: 0.8810980940723903\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5791\n",
            "     suicide       0.90      0.87      0.88      5813\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 12: 6.454s\n",
            "Predict time of fold 12: 6.475s\n",
            "Accuracy of fold 12: 0.8767666321957945\n",
            "Precision of fold 12: 0.8857963727778775\n",
            "Recall of fold 12: 0.8613584773878121\n",
            "F-measure of fold 12: 0.8734065155807366\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5877\n",
            "     suicide       0.89      0.86      0.87      5727\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 13: 6.988s\n",
            "Predict time of fold 13: 7.01s\n",
            "Accuracy of fold 13: 0.8818510858324715\n",
            "Precision of fold 13: 0.8890658174097664\n",
            "Recall of fold 13: 0.8710348413936557\n",
            "F-measure of fold 13: 0.8799579721565538\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.88      0.89      0.88      5835\n",
            "     suicide       0.89      0.87      0.88      5769\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 14: 6.278s\n",
            "Predict time of fold 14: 6.298s\n",
            "Accuracy of fold 14: 0.8772836952774905\n",
            "Precision of fold 14: 0.8907337975361542\n",
            "Recall of fold 14: 0.8600241337700396\n",
            "F-measure of fold 14: 0.875109629889493\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5803\n",
            "     suicide       0.89      0.86      0.88      5801\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 15: 6.609s\n",
            "Predict time of fold 15: 6.63s\n",
            "Accuracy of fold 15: 0.8751292657704239\n",
            "Precision of fold 15: 0.8913743736578382\n",
            "Recall of fold 15: 0.855400996050146\n",
            "F-measure of fold 15: 0.8730172640434668\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.90      0.88      5781\n",
            "     suicide       0.89      0.86      0.87      5823\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 16: 6.693s\n",
            "Predict time of fold 16: 6.717s\n",
            "Accuracy of fold 16: 0.8793519476042744\n",
            "Precision of fold 16: 0.8958220293000543\n",
            "Recall of fold 16: 0.8573654145750389\n",
            "F-measure of fold 16: 0.8761719441004776\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.90      0.88      5827\n",
            "     suicide       0.90      0.86      0.88      5777\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 17: 6.456s\n",
            "Predict time of fold 17: 6.478s\n",
            "Accuracy of fold 17: 0.8796966563254051\n",
            "Precision of fold 17: 0.889273356401384\n",
            "Recall of fold 17: 0.8717774762550882\n",
            "F-measure of fold 17: 0.880438506337787\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5708\n",
            "     suicide       0.89      0.87      0.88      5896\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 18: 7.037s\n",
            "Predict time of fold 18: 7.058s\n",
            "Accuracy of fold 18: 0.8815823493923985\n",
            "Precision of fold 18: 0.8948557525199861\n",
            "Recall of fold 18: 0.8700574518418385\n",
            "F-measure of fold 18: 0.8822823851953393\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5685\n",
            "     suicide       0.89      0.87      0.88      5918\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 19: 6.618s\n",
            "Predict time of fold 19: 6.639s\n",
            "Accuracy of fold 19: 0.8741704731534948\n",
            "Precision of fold 19: 0.8879526712083184\n",
            "Recall of fold 19: 0.85573600552868\n",
            "F-measure of fold 19: 0.8715467182825971\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5815\n",
            "     suicide       0.89      0.86      0.87      5788\n",
            "\n",
            "    accuracy                           0.87     11603\n",
            "   macro avg       0.87      0.87      0.87     11603\n",
            "weighted avg       0.87      0.87      0.87     11603\n",
            "\n",
            "Predict time of fold 20: 6.675s\n",
            "Predict time of fold 20: 6.696s\n",
            "Accuracy of fold 20: 0.8753770576575024\n",
            "Precision of fold 20: 0.8903408087586085\n",
            "Recall of fold 20: 0.8593829896028635\n",
            "F-measure of fold 20: 0.874588031222897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5736\n",
            "     suicide       0.89      0.86      0.87      5867\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 21: 6.733s\n",
            "Predict time of fold 21: 6.755s\n",
            "Accuracy of fold 21: 0.8803860737676663\n",
            "Precision of fold 21: 0.8949084979162892\n",
            "Recall of fold 21: 0.859404906907952\n",
            "F-measure of fold 21: 0.8767974436357181\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5857\n",
            "     suicide       0.89      0.86      0.88      5747\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 22: 6.675s\n",
            "Predict time of fold 22: 6.697s\n",
            "Accuracy of fold 22: 0.8800413650465356\n",
            "Precision of fold 22: 0.8925075636234205\n",
            "Recall of fold 22: 0.8642081681888678\n",
            "F-measure of fold 22: 0.8781299247067064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5801\n",
            "     suicide       0.89      0.86      0.88      5803\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 23: 6.791s\n",
            "Predict time of fold 23: 6.812s\n",
            "Accuracy of fold 23: 0.8735780765253361\n",
            "Precision of fold 23: 0.8857446055889636\n",
            "Recall of fold 23: 0.8591525132955911\n",
            "F-measure of fold 23: 0.8722459287642603\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.87      5775\n",
            "     suicide       0.89      0.86      0.87      5829\n",
            "\n",
            "    accuracy                           0.87     11604\n",
            "   macro avg       0.87      0.87      0.87     11604\n",
            "weighted avg       0.87      0.87      0.87     11604\n",
            "\n",
            "Predict time of fold 24: 7.107s\n",
            "Predict time of fold 24: 7.128s\n",
            "Accuracy of fold 24: 0.8796104791451224\n",
            "Precision of fold 24: 0.8931230060262318\n",
            "Recall of fold 24: 0.8638779358820504\n",
            "F-measure of fold 24: 0.8782570806100218\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5771\n",
            "     suicide       0.89      0.86      0.88      5833\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 25: 6.279s\n",
            "Predict time of fold 25: 6.301s\n",
            "Accuracy of fold 25: 0.8766804550155118\n",
            "Precision of fold 25: 0.8934890304317056\n",
            "Recall of fold 25: 0.8589896240857289\n",
            "F-measure of fold 25: 0.8758997485040326\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5725\n",
            "     suicide       0.89      0.86      0.88      5879\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 26: 6.943s\n",
            "Predict time of fold 26: 6.965s\n",
            "Accuracy of fold 26: 0.8798690106859703\n",
            "Precision of fold 26: 0.8896539548022598\n",
            "Recall of fold 26: 0.8675964187327824\n",
            "F-measure of fold 26: 0.878486750348675\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5796\n",
            "     suicide       0.89      0.87      0.88      5808\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 27: 6.969s\n",
            "Predict time of fold 27: 6.991s\n",
            "Accuracy of fold 27: 0.8776284039986212\n",
            "Precision of fold 27: 0.8886932018316308\n",
            "Recall of fold 27: 0.8649297223174495\n",
            "F-measure of fold 27: 0.8766504517025712\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5770\n",
            "     suicide       0.89      0.86      0.88      5834\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 28: 6.96s\n",
            "Predict time of fold 28: 6.98s\n",
            "Accuracy of fold 28: 0.8807205033181074\n",
            "Precision of fold 28: 0.8959180067149674\n",
            "Recall of fold 28: 0.8644501278772379\n",
            "F-measure of fold 28: 0.8799028115237765\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5738\n",
            "     suicide       0.90      0.86      0.88      5865\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 29: 6.618s\n",
            "Predict time of fold 29: 6.64s\n",
            "Accuracy of fold 29: 0.8780487804878049\n",
            "Precision of fold 29: 0.8839857651245552\n",
            "Recall of fold 29: 0.8668644215669168\n",
            "F-measure of fold 29: 0.875341379614131\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5872\n",
            "     suicide       0.88      0.87      0.88      5731\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 30: 6.563s\n",
            "Predict time of fold 30: 6.584s\n",
            "Accuracy of fold 30: 0.8758079806946479\n",
            "Precision of fold 30: 0.8914935180252176\n",
            "Recall of fold 30: 0.8581196581196581\n",
            "F-measure of fold 30: 0.8744882849925965\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5753\n",
            "     suicide       0.89      0.86      0.87      5850\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "1-Gram Linear Support Vector Classification Average: {'average_train_time': 6.782700000000001, 'average_predict_time': 6.804633333333332, 'average_accuracy': 0.8783433720625351, 'average_precision': 0.8909664975085206, 'average_recall': 0.8629486425559576, 'average_f_measure': 0.8767248340171574}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_gram_svc_average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg-AEDvtMpyp",
        "outputId": "b4475691-ec1c-4604-c60d-077fd3f712cf"
      },
      "id": "Jg-AEDvtMpyp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_accuracy': 0.8783433720625351,\n",
              " 'average_f_measure': 0.8767248340171574,\n",
              " 'average_precision': 0.8909664975085206,\n",
              " 'average_predict_time': 6.804633333333332,\n",
              " 'average_recall': 0.8629486425559576,\n",
              " 'average_train_time': 6.782700000000001}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_gram_svc_average = measure_accuracy(linearSVC, two_gram_vectorizer, dense=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9Xsr479Z4JO",
        "outputId": "dc987cf5-a917-4b11-da34-03c845cac9b4"
      },
      "id": "A9Xsr479Z4JO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict time of fold 1: 3.176s\n",
            "Predict time of fold 1: 3.189s\n",
            "Accuracy of fold 1: 0.8420372285418821\n",
            "Precision of fold 1: 0.8578343492291144\n",
            "Recall of fold 1: 0.8214592274678112\n",
            "F-measure of fold 1: 0.8392528282031044\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.86      0.84      5779\n",
            "     suicide       0.86      0.82      0.84      5825\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 2: 3.347s\n",
            "Predict time of fold 2: 3.359s\n",
            "Accuracy of fold 2: 0.8354877628403998\n",
            "Precision of fold 2: 0.855569829326482\n",
            "Recall of fold 2: 0.8060165975103735\n",
            "F-measure of fold 2: 0.8300543042820262\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5820\n",
            "     suicide       0.86      0.81      0.83      5784\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 3: 3.137s\n",
            "Predict time of fold 3: 3.15s\n",
            "Accuracy of fold 3: 0.8396242674939676\n",
            "Precision of fold 3: 0.8633015987066642\n",
            "Recall of fold 3: 0.813748730104978\n",
            "F-measure of fold 3: 0.8377930794038175\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5698\n",
            "     suicide       0.86      0.81      0.84      5906\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 4: 3.2s\n",
            "Predict time of fold 4: 3.213s\n",
            "Accuracy of fold 4: 0.8429851775249914\n",
            "Precision of fold 4: 0.8630087860857092\n",
            "Recall of fold 4: 0.8197921989439618\n",
            "F-measure of fold 4: 0.8408455625436758\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5733\n",
            "     suicide       0.86      0.82      0.84      5871\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 5: 3.229s\n",
            "Predict time of fold 5: 3.242s\n",
            "Accuracy of fold 5: 0.8418648741813168\n",
            "Precision of fold 5: 0.860549790642636\n",
            "Recall of fold 5: 0.8155624568668047\n",
            "F-measure of fold 5: 0.8374523872796528\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5808\n",
            "     suicide       0.86      0.82      0.84      5796\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 6: 3.488s\n",
            "Predict time of fold 6: 3.5s\n",
            "Accuracy of fold 6: 0.8376421923474664\n",
            "Precision of fold 6: 0.8576134699853587\n",
            "Recall of fold 6: 0.8090469613259669\n",
            "F-measure of fold 6: 0.8326226012793178\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5812\n",
            "     suicide       0.86      0.81      0.83      5792\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 7: 3.635s\n",
            "Predict time of fold 7: 3.648s\n",
            "Accuracy of fold 7: 0.8384177869700103\n",
            "Precision of fold 7: 0.865089398591295\n",
            "Recall of fold 7: 0.8093950659006421\n",
            "F-measure of fold 7: 0.8363160192055872\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.87      0.84      5686\n",
            "     suicide       0.87      0.81      0.84      5918\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 8: 3.263s\n",
            "Predict time of fold 8: 3.277s\n",
            "Accuracy of fold 8: 0.8402999224338533\n",
            "Precision of fold 8: 0.8617630752171502\n",
            "Recall of fold 8: 0.8084257975034674\n",
            "F-measure of fold 8: 0.8342427766347617\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5835\n",
            "     suicide       0.86      0.81      0.83      5768\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 9: 3.524s\n",
            "Predict time of fold 9: 3.537s\n",
            "Accuracy of fold 9: 0.8354735844178229\n",
            "Precision of fold 9: 0.8524318349299926\n",
            "Recall of fold 9: 0.8068003487358326\n",
            "F-measure of fold 9: 0.828988623129983\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5868\n",
            "     suicide       0.85      0.81      0.83      5735\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 10: 3.384s\n",
            "Predict time of fold 10: 3.396s\n",
            "Accuracy of fold 10: 0.8390933379298458\n",
            "Precision of fold 10: 0.8603495860165593\n",
            "Recall of fold 10: 0.8084370677731674\n",
            "F-measure of fold 10: 0.8335858810945717\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5819\n",
            "     suicide       0.86      0.81      0.83      5784\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 11: 3.529s\n",
            "Predict time of fold 11: 3.542s\n",
            "Accuracy of fold 11: 0.8421234057221648\n",
            "Precision of fold 11: 0.8636945002740727\n",
            "Recall of fold 11: 0.8131773610872183\n",
            "F-measure of fold 11: 0.8376749955697324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5791\n",
            "     suicide       0.86      0.81      0.84      5813\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 12: 3.56s\n",
            "Predict time of fold 12: 3.573s\n",
            "Accuracy of fold 12: 0.8441916580489487\n",
            "Precision of fold 12: 0.860666298545923\n",
            "Recall of fold 12: 0.8164833246027589\n",
            "F-measure of fold 12: 0.8379928315412186\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5877\n",
            "     suicide       0.86      0.82      0.84      5727\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.85      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 13: 3.329s\n",
            "Predict time of fold 13: 3.342s\n",
            "Accuracy of fold 13: 0.8420372285418821\n",
            "Precision of fold 13: 0.8572984749455338\n",
            "Recall of fold 13: 0.8185127405096204\n",
            "F-measure of fold 13: 0.8374567704176643\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5835\n",
            "     suicide       0.86      0.82      0.84      5769\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 14: 3.518s\n",
            "Predict time of fold 14: 3.531s\n",
            "Accuracy of fold 14: 0.8361771802826612\n",
            "Precision of fold 14: 0.8587196467991169\n",
            "Recall of fold 14: 0.8046888467505603\n",
            "F-measure of fold 14: 0.8308267331138204\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5803\n",
            "     suicide       0.86      0.80      0.83      5801\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 15: 3.377s\n",
            "Predict time of fold 15: 3.389s\n",
            "Accuracy of fold 15: 0.839796621854533\n",
            "Precision of fold 15: 0.8599709407918634\n",
            "Recall of fold 15: 0.8131547312381934\n",
            "F-measure of fold 15: 0.8359078471180158\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5781\n",
            "     suicide       0.86      0.81      0.84      5823\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 16: 3.467s\n",
            "Predict time of fold 16: 3.482s\n",
            "Accuracy of fold 16: 0.8386763185108583\n",
            "Precision of fold 16: 0.8595102191125022\n",
            "Recall of fold 16: 0.8080318504414056\n",
            "F-measure of fold 16: 0.8329764453961456\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5827\n",
            "     suicide       0.86      0.81      0.83      5777\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 17: 3.161s\n",
            "Predict time of fold 17: 3.175s\n",
            "Accuracy of fold 17: 0.8362633574629438\n",
            "Precision of fold 17: 0.859740727403673\n",
            "Recall of fold 17: 0.8098710990502035\n",
            "F-measure of fold 17: 0.8340611353711789\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.86      0.84      5708\n",
            "     suicide       0.86      0.81      0.83      5896\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 18: 3.575s\n",
            "Predict time of fold 18: 3.588s\n",
            "Accuracy of fold 18: 0.8413341377230027\n",
            "Precision of fold 18: 0.864343163538874\n",
            "Recall of fold 18: 0.8171679621493748\n",
            "F-measure of fold 18: 0.8400938070007818\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5685\n",
            "     suicide       0.86      0.82      0.84      5918\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 19: 3.35s\n",
            "Predict time of fold 19: 3.363s\n",
            "Accuracy of fold 19: 0.8388347841075584\n",
            "Precision of fold 19: 0.8603752759381899\n",
            "Recall of fold 19: 0.8080511402902557\n",
            "F-measure of fold 19: 0.8333927298645759\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5815\n",
            "     suicide       0.86      0.81      0.83      5788\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 20: 3.139s\n",
            "Predict time of fold 20: 3.152s\n",
            "Accuracy of fold 20: 0.8329742308023786\n",
            "Precision of fold 20: 0.8526296894632921\n",
            "Recall of fold 20: 0.8096130901653316\n",
            "F-measure of fold 20: 0.8305647840531561\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.86      0.84      5736\n",
            "     suicide       0.85      0.81      0.83      5867\n",
            "\n",
            "    accuracy                           0.83     11603\n",
            "   macro avg       0.83      0.83      0.83     11603\n",
            "weighted avg       0.83      0.83      0.83     11603\n",
            "\n",
            "Predict time of fold 21: 3.648s\n",
            "Predict time of fold 21: 3.66s\n",
            "Accuracy of fold 21: 0.8343674594967253\n",
            "Precision of fold 21: 0.8518859245630175\n",
            "Recall of fold 21: 0.8056377240299286\n",
            "F-measure of fold 21: 0.8281166159899839\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5857\n",
            "     suicide       0.85      0.81      0.83      5747\n",
            "\n",
            "    accuracy                           0.83     11604\n",
            "   macro avg       0.84      0.83      0.83     11604\n",
            "weighted avg       0.84      0.83      0.83     11604\n",
            "\n",
            "Predict time of fold 22: 3.415s\n",
            "Predict time of fold 22: 3.428s\n",
            "Accuracy of fold 22: 0.8404860392967942\n",
            "Precision of fold 22: 0.860979174278407\n",
            "Recall of fold 22: 0.8121661209719111\n",
            "F-measure of fold 22: 0.8358606012237297\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5801\n",
            "     suicide       0.86      0.81      0.84      5803\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 23: 3.092s\n",
            "Predict time of fold 23: 3.105s\n",
            "Accuracy of fold 23: 0.8372974836263357\n",
            "Precision of fold 23: 0.8591215600510297\n",
            "Recall of fold 23: 0.8087150454623434\n",
            "F-measure of fold 23: 0.8331565924354895\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5775\n",
            "     suicide       0.86      0.81      0.83      5829\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 24: 3.123s\n",
            "Predict time of fold 24: 3.135s\n",
            "Accuracy of fold 24: 0.8381592554291624\n",
            "Precision of fold 24: 0.8613192033619587\n",
            "Recall of fold 24: 0.8081604663123607\n",
            "F-measure of fold 24: 0.8338935078719264\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5771\n",
            "     suicide       0.86      0.81      0.83      5833\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 25: 3.442s\n",
            "Predict time of fold 25: 3.455s\n",
            "Accuracy of fold 25: 0.8432437090658393\n",
            "Precision of fold 25: 0.8631484794275492\n",
            "Recall of fold 25: 0.8207178091512162\n",
            "F-measure of fold 25: 0.8413985526201064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5725\n",
            "     suicide       0.86      0.82      0.84      5879\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 26: 3.188s\n",
            "Predict time of fold 26: 3.201s\n",
            "Accuracy of fold 26: 0.8402275077559462\n",
            "Precision of fold 26: 0.8576338639652678\n",
            "Recall of fold 26: 0.8162878787878788\n",
            "F-measure of fold 26: 0.8364502470007058\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5796\n",
            "     suicide       0.86      0.82      0.84      5808\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 27: 3.399s\n",
            "Predict time of fold 27: 3.412s\n",
            "Accuracy of fold 27: 0.8419510513615994\n",
            "Precision of fold 27: 0.8645643456069997\n",
            "Recall of fold 27: 0.8129928008227632\n",
            "F-measure of fold 27: 0.8379858657243817\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5770\n",
            "     suicide       0.86      0.81      0.84      5834\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 28: 4.239s\n",
            "Predict time of fold 28: 4.251s\n",
            "Accuracy of fold 28: 0.8412479531155735\n",
            "Precision of fold 28: 0.8623671410556657\n",
            "Recall of fold 28: 0.8161977834612105\n",
            "F-measure of fold 28: 0.83864751226349\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5738\n",
            "     suicide       0.86      0.82      0.84      5865\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 29: 3.55s\n",
            "Predict time of fold 29: 3.562s\n",
            "Accuracy of fold 29: 0.8407308454709989\n",
            "Precision of fold 29: 0.8572217111315548\n",
            "Recall of fold 29: 0.8129471296457861\n",
            "F-measure of fold 29: 0.8344975819451907\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5872\n",
            "     suicide       0.86      0.81      0.83      5731\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 30: 3.397s\n",
            "Predict time of fold 30: 3.41s\n",
            "Accuracy of fold 30: 0.83478410755839\n",
            "Precision of fold 30: 0.8593093367440161\n",
            "Recall of fold 30: 0.803931623931624\n",
            "F-measure of fold 30: 0.8306985781153405\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.87      0.84      5753\n",
            "     suicide       0.86      0.80      0.83      5850\n",
            "\n",
            "    accuracy                           0.83     11603\n",
            "   macro avg       0.84      0.84      0.83     11603\n",
            "weighted avg       0.84      0.83      0.83     11603\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_gram_svc_average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFTCwz_eXnuH",
        "outputId": "e5252e8b-2bae-4e41-fc75-afce38117936"
      },
      "id": "cFTCwz_eXnuH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_accuracy': 0.839261015663862,\n",
              " 'average_f_measure': 0.8350935932564377,\n",
              " 'average_precision': 0.8597337131909825,\n",
              " 'average_predict_time': 3.4089000000000005,\n",
              " 'average_recall': 0.8118396993664982,\n",
              " 'average_train_time': 3.396033333333334}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "L7A05hQXbK6U"
      },
      "id": "L7A05hQXbK6U"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "OX0qAdYEbPkn"
      },
      "id": "OX0qAdYEbPkn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_gram_log_average = measure_accuracy(LogisticRegression(), one_gram_vectorizer, dense=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcHgenDYbRyW",
        "outputId": "e5c4e571-9a86-448b-bbd7-a1c09a1d8736"
      },
      "id": "XcHgenDYbRyW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict time of fold 1: 1.358s\n",
            "Predict time of fold 1: 1.367s\n",
            "Accuracy of fold 1: 0.8816787314719062\n",
            "Precision of fold 1: 0.8959445037353255\n",
            "Recall of fold 1: 0.8647210300429184\n",
            "F-measure of fold 1: 0.8800559098453743\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5779\n",
            "     suicide       0.90      0.86      0.88      5825\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 2: 1.357s\n",
            "Predict time of fold 2: 1.367s\n",
            "Accuracy of fold 2: 0.8749569114098587\n",
            "Precision of fold 2: 0.887775192410954\n",
            "Recall of fold 2: 0.8575380359612724\n",
            "F-measure of fold 2: 0.8723946882420193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5820\n",
            "     suicide       0.89      0.86      0.87      5784\n",
            "\n",
            "    accuracy                           0.87     11604\n",
            "   macro avg       0.88      0.87      0.87     11604\n",
            "weighted avg       0.88      0.87      0.87     11604\n",
            "\n",
            "Predict time of fold 3: 1.235s\n",
            "Predict time of fold 3: 1.249s\n",
            "Accuracy of fold 3: 0.8808169596690796\n",
            "Precision of fold 3: 0.8961289192503066\n",
            "Recall of fold 3: 0.8662377243481205\n",
            "F-measure of fold 3: 0.8809298321136462\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5698\n",
            "     suicide       0.90      0.87      0.88      5906\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 4: 1.277s\n",
            "Predict time of fold 4: 1.286s\n",
            "Accuracy of fold 4: 0.8821096173733195\n",
            "Precision of fold 4: 0.8941011727638719\n",
            "Recall of fold 4: 0.8700391756089252\n",
            "F-measure of fold 4: 0.8819060773480663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5733\n",
            "     suicide       0.89      0.87      0.88      5871\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 5: 1.521s\n",
            "Predict time of fold 5: 1.531s\n",
            "Accuracy of fold 5: 0.8808169596690796\n",
            "Precision of fold 5: 0.8904618651566094\n",
            "Recall of fold 5: 0.8681849551414769\n",
            "F-measure of fold 5: 0.8791823185114003\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5808\n",
            "     suicide       0.89      0.87      0.88      5796\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 6: 1.366s\n",
            "Predict time of fold 6: 1.375s\n",
            "Accuracy of fold 6: 0.8772836952774905\n",
            "Precision of fold 6: 0.8906976744186047\n",
            "Recall of fold 6: 0.8596339779005525\n",
            "F-measure of fold 6: 0.8748901774732034\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5812\n",
            "     suicide       0.89      0.86      0.87      5792\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 7: 1.17s\n",
            "Predict time of fold 7: 1.179s\n",
            "Accuracy of fold 7: 0.8758186832126853\n",
            "Precision of fold 7: 0.8925127126073996\n",
            "Recall of fold 7: 0.8600878675228117\n",
            "F-measure of fold 7: 0.8760003442044575\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5686\n",
            "     suicide       0.89      0.86      0.88      5918\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 8: 1.434s\n",
            "Predict time of fold 8: 1.443s\n",
            "Accuracy of fold 8: 0.8767560113763682\n",
            "Precision of fold 8: 0.887459807073955\n",
            "Recall of fold 8: 0.8613037447988904\n",
            "F-measure of fold 8: 0.8741861692767904\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5835\n",
            "     suicide       0.89      0.86      0.87      5768\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 9: 1.228s\n",
            "Predict time of fold 9: 1.237s\n",
            "Accuracy of fold 9: 0.8789968111695251\n",
            "Precision of fold 9: 0.8866273879664346\n",
            "Recall of fold 9: 0.8659110723626853\n",
            "F-measure of fold 9: 0.8761467889908257\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5868\n",
            "     suicide       0.89      0.87      0.88      5735\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 10: 1.389s\n",
            "Predict time of fold 10: 1.399s\n",
            "Accuracy of fold 10: 0.873567180901491\n",
            "Precision of fold 10: 0.8860668932212484\n",
            "Recall of fold 10: 0.8565006915629322\n",
            "F-measure of fold 10: 0.8710329670329671\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5819\n",
            "     suicide       0.89      0.86      0.87      5784\n",
            "\n",
            "    accuracy                           0.87     11603\n",
            "   macro avg       0.87      0.87      0.87     11603\n",
            "weighted avg       0.87      0.87      0.87     11603\n",
            "\n",
            "Predict time of fold 11: 1.51s\n",
            "Predict time of fold 11: 1.519s\n",
            "Accuracy of fold 11: 0.881247845570493\n",
            "Precision of fold 11: 0.8942222222222223\n",
            "Recall of fold 11: 0.8653019095131601\n",
            "F-measure of fold 11: 0.8795243923762895\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5791\n",
            "     suicide       0.89      0.87      0.88      5813\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 12: 1.35s\n",
            "Predict time of fold 12: 1.36s\n",
            "Accuracy of fold 12: 0.8774560496380558\n",
            "Precision of fold 12: 0.8863758750673129\n",
            "Recall of fold 12: 0.8622315348349922\n",
            "F-measure of fold 12: 0.874137015400956\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5877\n",
            "     suicide       0.89      0.86      0.87      5727\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 13: 1.262s\n",
            "Predict time of fold 13: 1.271s\n",
            "Accuracy of fold 13: 0.880730782488797\n",
            "Precision of fold 13: 0.8879844275349495\n",
            "Recall of fold 13: 0.8698214595250476\n",
            "F-measure of fold 13: 0.8788091068301226\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5835\n",
            "     suicide       0.89      0.87      0.88      5769\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 14: 1.276s\n",
            "Predict time of fold 14: 1.286s\n",
            "Accuracy of fold 14: 0.8767666321957945\n",
            "Precision of fold 14: 0.8901981788966256\n",
            "Recall of fold 14: 0.8595069815549043\n",
            "F-measure of fold 14: 0.8745834064199263\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5803\n",
            "     suicide       0.89      0.86      0.87      5801\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 15: 1.202s\n",
            "Predict time of fold 15: 1.211s\n",
            "Accuracy of fold 15: 0.8761633919338159\n",
            "Precision of fold 15: 0.8923076923076924\n",
            "Recall of fold 15: 0.856603125536665\n",
            "F-measure of fold 15: 0.8740909489179006\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.90      0.88      5781\n",
            "     suicide       0.89      0.86      0.87      5823\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 16: 1.255s\n",
            "Predict time of fold 16: 1.264s\n",
            "Accuracy of fold 16: 0.8785763529817304\n",
            "Precision of fold 16: 0.8956521739130435\n",
            "Recall of fold 16: 0.8558075125497663\n",
            "F-measure of fold 16: 0.8752766221120651\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.90      0.88      5827\n",
            "     suicide       0.90      0.86      0.88      5777\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 17: 1.563s\n",
            "Predict time of fold 17: 1.572s\n",
            "Accuracy of fold 17: 0.8794381247845571\n",
            "Precision of fold 17: 0.8892158559806128\n",
            "Recall of fold 17: 0.871268656716418\n",
            "F-measure of fold 17: 0.8801507752934122\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5708\n",
            "     suicide       0.89      0.87      0.88      5896\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 18: 1.32s\n",
            "Predict time of fold 18: 1.329s\n",
            "Accuracy of fold 18: 0.88046194949582\n",
            "Precision of fold 18: 0.8939314901756217\n",
            "Recall of fold 18: 0.8687056437985806\n",
            "F-measure of fold 18: 0.8811380581026652\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5685\n",
            "     suicide       0.89      0.87      0.88      5918\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 19: 1.122s\n",
            "Predict time of fold 19: 1.131s\n",
            "Accuracy of fold 19: 0.8734809962940618\n",
            "Precision of fold 19: 0.8872355683040516\n",
            "Recall of fold 19: 0.8550449205252246\n",
            "F-measure of fold 19: 0.8708428646841457\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5815\n",
            "     suicide       0.89      0.86      0.87      5788\n",
            "\n",
            "    accuracy                           0.87     11603\n",
            "   macro avg       0.87      0.87      0.87     11603\n",
            "weighted avg       0.87      0.87      0.87     11603\n",
            "\n",
            "Predict time of fold 20: 1.589s\n",
            "Predict time of fold 20: 1.598s\n",
            "Accuracy of fold 20: 0.8764112729466518\n",
            "Precision of fold 20: 0.8901601830663616\n",
            "Recall of fold 20: 0.8619396625191751\n",
            "F-measure of fold 20: 0.8758226532732941\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5736\n",
            "     suicide       0.89      0.86      0.88      5867\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 21: 1.313s\n",
            "Predict time of fold 21: 1.322s\n",
            "Accuracy of fold 21: 0.8797828335056876\n",
            "Precision of fold 21: 0.8942028985507247\n",
            "Recall of fold 21: 0.8588828954236993\n",
            "F-measure of fold 21: 0.8761870950563593\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5857\n",
            "     suicide       0.89      0.86      0.88      5747\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 22: 1.581s\n",
            "Predict time of fold 22: 1.59s\n",
            "Accuracy of fold 22: 0.8814201999310582\n",
            "Precision of fold 22: 0.8939313045025805\n",
            "Recall of fold 22: 0.8655867654661382\n",
            "F-measure of fold 22: 0.8795307301698476\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.90      0.88      5801\n",
            "     suicide       0.89      0.87      0.88      5803\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 23: 1.475s\n",
            "Predict time of fold 23: 1.484s\n",
            "Accuracy of fold 23: 0.8734057221647707\n",
            "Precision of fold 23: 0.8857041755130927\n",
            "Recall of fold 23: 0.8588094012695145\n",
            "F-measure of fold 23: 0.8720494730424179\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.87      5775\n",
            "     suicide       0.89      0.86      0.87      5829\n",
            "\n",
            "    accuracy                           0.87     11604\n",
            "   macro avg       0.87      0.87      0.87     11604\n",
            "weighted avg       0.87      0.87      0.87     11604\n",
            "\n",
            "Predict time of fold 24: 1.446s\n",
            "Predict time of fold 24: 1.455s\n",
            "Accuracy of fold 24: 0.8793519476042744\n",
            "Precision of fold 24: 0.8925092969718434\n",
            "Recall of fold 24: 0.8640493742499571\n",
            "F-measure of fold 24: 0.8780487804878049\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5771\n",
            "     suicide       0.89      0.86      0.88      5833\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 25: 1.297s\n",
            "Predict time of fold 25: 1.306s\n",
            "Accuracy of fold 25: 0.8767666321957945\n",
            "Precision of fold 25: 0.8942052099946837\n",
            "Recall of fold 25: 0.8583092362646708\n",
            "F-measure of fold 25: 0.875889602499566\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.90      0.88      5725\n",
            "     suicide       0.89      0.86      0.88      5879\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 26: 1.346s\n",
            "Predict time of fold 26: 1.355s\n",
            "Accuracy of fold 26: 0.8796104791451224\n",
            "Precision of fold 26: 0.8889084817492505\n",
            "Recall of fold 26: 0.8679407713498623\n",
            "F-measure of fold 26: 0.8782995034410663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5796\n",
            "     suicide       0.89      0.87      0.88      5808\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 27: 1.153s\n",
            "Predict time of fold 27: 1.162s\n",
            "Accuracy of fold 27: 0.8759910375732506\n",
            "Precision of fold 27: 0.8870882508367095\n",
            "Recall of fold 27: 0.8632156324991429\n",
            "F-measure of fold 27: 0.8749891408218227\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5770\n",
            "     suicide       0.89      0.86      0.87      5834\n",
            "\n",
            "    accuracy                           0.88     11604\n",
            "   macro avg       0.88      0.88      0.88     11604\n",
            "weighted avg       0.88      0.88      0.88     11604\n",
            "\n",
            "Predict time of fold 28: 1.388s\n",
            "Predict time of fold 28: 1.397s\n",
            "Accuracy of fold 28: 0.8789968111695251\n",
            "Precision of fold 28: 0.8938725057390076\n",
            "Recall of fold 28: 0.8630861040068201\n",
            "F-measure of fold 28: 0.8782095766828591\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.90      0.88      5738\n",
            "     suicide       0.89      0.86      0.88      5865\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 29: 1.278s\n",
            "Predict time of fold 29: 1.287s\n",
            "Accuracy of fold 29: 0.8792553649918124\n",
            "Precision of fold 29: 0.8846837242359631\n",
            "Recall of fold 29: 0.8687838073634618\n",
            "F-measure of fold 29: 0.8766616779646095\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.87      0.89      0.88      5872\n",
            "     suicide       0.88      0.87      0.88      5731\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n",
            "Predict time of fold 30: 1.363s\n",
            "Predict time of fold 30: 1.372s\n",
            "Accuracy of fold 30: 0.8760665345169353\n",
            "Precision of fold 30: 0.8918294849023091\n",
            "Recall of fold 30: 0.8582905982905983\n",
            "F-measure of fold 30: 0.8747386759581882\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.86      0.89      0.88      5753\n",
            "     suicide       0.89      0.86      0.87      5850\n",
            "\n",
            "    accuracy                           0.88     11603\n",
            "   macro avg       0.88      0.88      0.88     11603\n",
            "weighted avg       0.88      0.88      0.88     11603\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_gram_log_average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSGDA5_5QPON",
        "outputId": "75ba6ef9-4ec5-4845-e9c2-4a5bf1bb9c8f"
      },
      "id": "iSGDA5_5QPON",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_accuracy': 0.8781394174219603,\n",
              " 'average_f_measure': 0.8765235124191355,\n",
              " 'average_precision': 0.890733170968979,\n",
              " 'average_predict_time': 1.3567999999999996,\n",
              " 'average_recall': 0.8627781422836129,\n",
              " 'average_train_time': 1.3474666666666661}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_gram_log_average = measure_accuracy(LogisticRegression(), two_gram_vectorizer, dense=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTCvCU8Jddc9",
        "outputId": "88338ec4-0059-4e50-980c-37ae2dff4133"
      },
      "id": "jTCvCU8Jddc9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict time of fold 1: 0.771s\n",
            "Predict time of fold 1: 0.775s\n",
            "Accuracy of fold 1: 0.8421234057221648\n",
            "Precision of fold 1: 0.8595353862776878\n",
            "Recall of fold 1: 0.8193991416309013\n",
            "F-measure of fold 1: 0.8389875197750043\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5779\n",
            "     suicide       0.86      0.82      0.84      5825\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 2: 0.782s\n",
            "Predict time of fold 2: 0.787s\n",
            "Accuracy of fold 2: 0.8353154084798345\n",
            "Precision of fold 2: 0.8552559163456247\n",
            "Recall of fold 2: 0.8060165975103735\n",
            "F-measure of fold 2: 0.8299065420560748\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5820\n",
            "     suicide       0.86      0.81      0.83      5784\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 3: 0.707s\n",
            "Predict time of fold 3: 0.712s\n",
            "Accuracy of fold 3: 0.8399689762150983\n",
            "Precision of fold 3: 0.8645777057446425\n",
            "Recall of fold 3: 0.8129021334236369\n",
            "F-measure of fold 3: 0.8379439741687755\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5698\n",
            "     suicide       0.86      0.81      0.84      5906\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 4: 0.745s\n",
            "Predict time of fold 4: 0.749s\n",
            "Accuracy of fold 4: 0.8428128231644261\n",
            "Precision of fold 4: 0.863481228668942\n",
            "Recall of fold 4: 0.8187702265372169\n",
            "F-measure of fold 4: 0.8405315614617939\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5733\n",
            "     suicide       0.86      0.82      0.84      5871\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 5: 0.72s\n",
            "Predict time of fold 5: 0.725s\n",
            "Accuracy of fold 5: 0.8404860392967942\n",
            "Precision of fold 5: 0.8598795840175151\n",
            "Recall of fold 5: 0.8131469979296067\n",
            "F-measure of fold 5: 0.8358606012237297\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5808\n",
            "     suicide       0.86      0.81      0.84      5796\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 6: 0.698s\n",
            "Predict time of fold 6: 0.703s\n",
            "Accuracy of fold 6: 0.8385901413305756\n",
            "Precision of fold 6: 0.8601360044109538\n",
            "Recall of fold 6: 0.8080110497237569\n",
            "F-measure of fold 6: 0.8332591471557019\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5812\n",
            "     suicide       0.86      0.81      0.83      5792\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 7: 0.749s\n",
            "Predict time of fold 7: 0.753s\n",
            "Accuracy of fold 7: 0.8384177869700103\n",
            "Precision of fold 7: 0.8661474370585039\n",
            "Recall of fold 7: 0.8080432578573843\n",
            "F-measure of fold 7: 0.8360870705481249\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.87      0.84      5686\n",
            "     suicide       0.87      0.81      0.84      5918\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 8: 0.727s\n",
            "Predict time of fold 8: 0.731s\n",
            "Accuracy of fold 8: 0.840041368611566\n",
            "Precision of fold 8: 0.8624907338769459\n",
            "Recall of fold 8: 0.8068654646324549\n",
            "F-measure of fold 8: 0.8337513436044428\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5835\n",
            "     suicide       0.86      0.81      0.83      5768\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 9: 0.705s\n",
            "Predict time of fold 9: 0.709s\n",
            "Accuracy of fold 9: 0.834697922950961\n",
            "Precision of fold 9: 0.8525771291335673\n",
            "Recall of fold 9: 0.8047079337401918\n",
            "F-measure of fold 9: 0.827951202009329\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5868\n",
            "     suicide       0.85      0.80      0.83      5735\n",
            "\n",
            "    accuracy                           0.83     11603\n",
            "   macro avg       0.84      0.83      0.83     11603\n",
            "weighted avg       0.84      0.83      0.83     11603\n",
            "\n",
            "Predict time of fold 10: 0.877s\n",
            "Predict time of fold 10: 0.882s\n",
            "Accuracy of fold 10: 0.8403861070412825\n",
            "Precision of fold 10: 0.8625968277388417\n",
            "Recall of fold 10: 0.8086099585062241\n",
            "F-measure of fold 10: 0.834731393896127\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5819\n",
            "     suicide       0.86      0.81      0.83      5784\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 11: 0.75s\n",
            "Predict time of fold 11: 0.755s\n",
            "Accuracy of fold 11: 0.8436745949672527\n",
            "Precision of fold 11: 0.8677579547544602\n",
            "Recall of fold 11: 0.8116291071735765\n",
            "F-measure of fold 11: 0.8387555555555555\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.88      0.85      5791\n",
            "     suicide       0.87      0.81      0.84      5813\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.85      0.84      0.84     11604\n",
            "weighted avg       0.85      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 12: 0.715s\n",
            "Predict time of fold 12: 0.72s\n",
            "Accuracy of fold 12: 0.844364012409514\n",
            "Precision of fold 12: 0.861249309010503\n",
            "Recall of fold 12: 0.8161341016238869\n",
            "F-measure of fold 12: 0.8380849919311458\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5877\n",
            "     suicide       0.86      0.82      0.84      5727\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.85      0.84      0.84     11604\n",
            "weighted avg       0.85      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 13: 0.953s\n",
            "Predict time of fold 13: 0.957s\n",
            "Accuracy of fold 13: 0.8421234057221648\n",
            "Precision of fold 13: 0.8582347588717015\n",
            "Recall of fold 13: 0.8174726989079563\n",
            "F-measure of fold 13: 0.8373579545454545\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5835\n",
            "     suicide       0.86      0.82      0.84      5769\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 14: 0.744s\n",
            "Predict time of fold 14: 0.748s\n",
            "Accuracy of fold 14: 0.8354015856601172\n",
            "Precision of fold 14: 0.8598113556500833\n",
            "Recall of fold 14: 0.8014135493880366\n",
            "F-measure of fold 14: 0.8295860099928622\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.87      0.84      5803\n",
            "     suicide       0.86      0.80      0.83      5801\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 15: 0.733s\n",
            "Predict time of fold 15: 0.738s\n",
            "Accuracy of fold 15: 0.8409169251982075\n",
            "Precision of fold 15: 0.8616112020367339\n",
            "Recall of fold 15: 0.8136699295895586\n",
            "F-measure of fold 15: 0.836954601660484\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5781\n",
            "     suicide       0.86      0.81      0.84      5823\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 16: 0.749s\n",
            "Predict time of fold 16: 0.754s\n",
            "Accuracy of fold 16: 0.8383316097897276\n",
            "Precision of fold 16: 0.8607360828555576\n",
            "Recall of fold 16: 0.8056084472909815\n",
            "F-measure of fold 16: 0.8322603719599427\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5827\n",
            "     suicide       0.86      0.81      0.83      5777\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 17: 0.636s\n",
            "Predict time of fold 17: 0.641s\n",
            "Accuracy of fold 17: 0.8359186487418132\n",
            "Precision of fold 17: 0.8596396396396396\n",
            "Recall of fold 17: 0.8091926729986432\n",
            "F-measure of fold 17: 0.8336536781408351\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.86      0.84      5708\n",
            "     suicide       0.86      0.81      0.83      5896\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 18: 0.622s\n",
            "Predict time of fold 18: 0.626s\n",
            "Accuracy of fold 18: 0.8419374299750064\n",
            "Precision of fold 18: 0.8656876790830945\n",
            "Recall of fold 18: 0.8168300101385603\n",
            "F-measure of fold 18: 0.8405494696574509\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5685\n",
            "     suicide       0.87      0.82      0.84      5918\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 19: 0.778s\n",
            "Predict time of fold 19: 0.783s\n",
            "Accuracy of fold 19: 0.8381453072481255\n",
            "Precision of fold 19: 0.8605680560678717\n",
            "Recall of fold 19: 0.8061506565307532\n",
            "F-measure of fold 19: 0.8324710080285459\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5815\n",
            "     suicide       0.86      0.81      0.83      5788\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 20: 0.811s\n",
            "Predict time of fold 20: 0.816s\n",
            "Accuracy of fold 20: 0.8342669999138154\n",
            "Precision of fold 20: 0.8548038862900323\n",
            "Recall of fold 20: 0.809783535026419\n",
            "F-measure of fold 20: 0.8316849015317287\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5736\n",
            "     suicide       0.85      0.81      0.83      5867\n",
            "\n",
            "    accuracy                           0.83     11603\n",
            "   macro avg       0.84      0.83      0.83     11603\n",
            "weighted avg       0.84      0.83      0.83     11603\n",
            "\n",
            "Predict time of fold 21: 0.709s\n",
            "Predict time of fold 21: 0.713s\n",
            "Accuracy of fold 21: 0.8344536366770079\n",
            "Precision of fold 21: 0.8528218369605312\n",
            "Recall of fold 21: 0.8045937010614234\n",
            "F-measure of fold 21: 0.828006088280061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5857\n",
            "     suicide       0.85      0.80      0.83      5747\n",
            "\n",
            "    accuracy                           0.83     11604\n",
            "   macro avg       0.84      0.83      0.83     11604\n",
            "weighted avg       0.84      0.83      0.83     11604\n",
            "\n",
            "Predict time of fold 22: 0.673s\n",
            "Predict time of fold 22: 0.679s\n",
            "Accuracy of fold 22: 0.8423819372630128\n",
            "Precision of fold 22: 0.8644534115920763\n",
            "Recall of fold 22: 0.8121661209719111\n",
            "F-measure of fold 22: 0.8374944469124833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5801\n",
            "     suicide       0.86      0.81      0.84      5803\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 23: 0.97s\n",
            "Predict time of fold 23: 0.975s\n",
            "Accuracy of fold 23: 0.8367804205446397\n",
            "Precision of fold 23: 0.8589673417259625\n",
            "Recall of fold 23: 0.8076857093841139\n",
            "F-measure of fold 23: 0.8325375773651637\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5775\n",
            "     suicide       0.86      0.81      0.83      5829\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 24: 0.723s\n",
            "Predict time of fold 24: 0.728s\n",
            "Accuracy of fold 24: 0.8374698379869011\n",
            "Precision of fold 24: 0.8619108747478452\n",
            "Recall of fold 24: 0.8057603291616664\n",
            "F-measure of fold 24: 0.8328903065745171\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5771\n",
            "     suicide       0.86      0.81      0.83      5833\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 25: 0.775s\n",
            "Predict time of fold 25: 0.779s\n",
            "Accuracy of fold 25: 0.8432437090658393\n",
            "Precision of fold 25: 0.865370770338373\n",
            "Recall of fold 25: 0.8178261609117197\n",
            "F-measure of fold 25: 0.8409269785745518\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5725\n",
            "     suicide       0.87      0.82      0.84      5879\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 26: 0.725s\n",
            "Predict time of fold 26: 0.73s\n",
            "Accuracy of fold 26: 0.840055153395381\n",
            "Precision of fold 26: 0.8581007611453425\n",
            "Recall of fold 26: 0.8152548209366391\n",
            "F-measure of fold 26: 0.8361292601094826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.86      0.84      5796\n",
            "     suicide       0.86      0.82      0.84      5808\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 27: 0.801s\n",
            "Predict time of fold 27: 0.806s\n",
            "Accuracy of fold 27: 0.8418648741813168\n",
            "Precision of fold 27: 0.8641413221635403\n",
            "Recall of fold 27: 0.8133356187864245\n",
            "F-measure of fold 27: 0.8379690949227374\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.85      5770\n",
            "     suicide       0.86      0.81      0.84      5834\n",
            "\n",
            "    accuracy                           0.84     11604\n",
            "   macro avg       0.84      0.84      0.84     11604\n",
            "weighted avg       0.84      0.84      0.84     11604\n",
            "\n",
            "Predict time of fold 28: 0.763s\n",
            "Predict time of fold 28: 0.77s\n",
            "Accuracy of fold 28: 0.8413341377230027\n",
            "Precision of fold 28: 0.8644927536231884\n",
            "Recall of fold 28: 0.8136402387041773\n",
            "F-measure of fold 28: 0.8382960035133948\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.82      0.87      0.84      5738\n",
            "     suicide       0.86      0.81      0.84      5865\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 29: 0.701s\n",
            "Predict time of fold 29: 0.706s\n",
            "Accuracy of fold 29: 0.8414203223304317\n",
            "Precision of fold 29: 0.8586175115207373\n",
            "Recall of fold 29: 0.8127726400279184\n",
            "F-measure of fold 29: 0.8350663320186448\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.83      0.87      0.85      5872\n",
            "     suicide       0.86      0.81      0.84      5731\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.84     11603\n",
            "\n",
            "Predict time of fold 30: 0.934s\n",
            "Predict time of fold 30: 0.939s\n",
            "Accuracy of fold 30: 0.8351288459881065\n",
            "Precision of fold 30: 0.8619231476374334\n",
            "Recall of fold 30: 0.8013675213675213\n",
            "F-measure of fold 30: 0.8305430064664717\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " non_suicide       0.81      0.87      0.84      5753\n",
            "     suicide       0.86      0.80      0.83      5850\n",
            "\n",
            "    accuracy                           0.84     11603\n",
            "   macro avg       0.84      0.84      0.84     11603\n",
            "weighted avg       0.84      0.84      0.83     11603\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_gram_log_average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWc3Qw9vXdE-",
        "outputId": "57e34328-c86f-4b5b-cb09-4e609acfeb77"
      },
      "id": "nWc3Qw9vXdE-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_accuracy': 0.8394017791521364,\n",
              " 'average_f_measure': 0.8350075997880206,\n",
              " 'average_precision': 0.860919253632931,\n",
              " 'average_predict_time': 0.7629666666666668,\n",
              " 'average_recall': 0.8106253443824547,\n",
              " 'average_train_time': 0.7582}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result Comparison"
      ],
      "metadata": {
        "id": "y49rZ_j7f8f7"
      },
      "id": "y49rZ_j7f8f7"
    },
    {
      "cell_type": "code",
      "source": [
        "average_comparison = {'Unigram Multinomial Naive Bayes': [one_gram_mnb_average['average_accuracy'],\n",
        "                                                        one_gram_mnb_average['average_precision'],\n",
        "                                                        one_gram_mnb_average['average_recall'],\n",
        "                                                        one_gram_mnb_average['average_f_measure'],\n",
        "                                                        one_gram_mnb_average['average_train_time'],\n",
        "                                                        one_gram_mnb_average['average_predict_time']],\n",
        "                    'Bigram Multinomial Naive Bayes': [two_gram_mnb_average['average_accuracy'],\n",
        "                                                        two_gram_mnb_average['average_precision'],\n",
        "                                                        two_gram_mnb_average['average_recall'],\n",
        "                                                        two_gram_mnb_average['average_f_measure'],\n",
        "                                                        two_gram_mnb_average['average_train_time'],\n",
        "                                                        two_gram_mnb_average['average_predict_time']],\n",
        "                    'Unigram Support Vector Classifier': [one_gram_svc_average['average_accuracy'],\n",
        "                                                        one_gram_svc_average['average_precision'],\n",
        "                                                        one_gram_svc_average['average_recall'],\n",
        "                                                        one_gram_svc_average['average_f_measure'],\n",
        "                                                        one_gram_svc_average['average_train_time'],\n",
        "                                                        one_gram_svc_average['average_predict_time']],\n",
        "                    'Bigram Support Vector Classifier': [two_gram_svc_average['average_accuracy'],\n",
        "                                                        two_gram_svc_average['average_precision'],\n",
        "                                                        two_gram_svc_average['average_recall'],\n",
        "                                                        two_gram_svc_average['average_f_measure'],\n",
        "                                                        two_gram_svc_average['average_train_time'],\n",
        "                                                        two_gram_svc_average['average_predict_time']],\n",
        "                    'Unigram Logistic Regression': [one_gram_log_average['average_accuracy'],\n",
        "                                                        one_gram_log_average['average_precision'],\n",
        "                                                        one_gram_log_average['average_recall'],\n",
        "                                                        one_gram_log_average['average_f_measure'],\n",
        "                                                        one_gram_log_average['average_train_time'],\n",
        "                                                        one_gram_log_average['average_predict_time']],\n",
        "                    'Bigram Logistic Regression': [two_gram_log_average['average_accuracy'],\n",
        "                                                        two_gram_log_average['average_precision'],\n",
        "                                                        two_gram_log_average['average_recall'],\n",
        "                                                        two_gram_log_average['average_f_measure'],\n",
        "                                                        two_gram_log_average['average_train_time'],\n",
        "                                                        two_gram_log_average['average_predict_time']]}\n",
        "average_comparison_df = pd.DataFrame.from_dict(average_comparison, orient='index', columns=['Accuracy', 'Precision', 'Recall', 'F-Measure', 'Train Time', 'Prediction Time'])\n",
        "                                               \n",
        "average_comparison_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "qAPV-cvFZmNC",
        "outputId": "2ad19f97-c64e-47b9-e929-673b04903914"
      },
      "id": "qAPV-cvFZmNC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f7a94f61-e799-4db9-a620-2a80dfc56764\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F-Measure</th>\n",
              "      <th>Train Time</th>\n",
              "      <th>Prediction Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unigram Multinomial Naive Bayes</th>\n",
              "      <td>0.805131</td>\n",
              "      <td>0.775404</td>\n",
              "      <td>0.860603</td>\n",
              "      <td>0.815769</td>\n",
              "      <td>0.066133</td>\n",
              "      <td>0.081700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bigram Multinomial Naive Bayes</th>\n",
              "      <td>0.659902</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.805630</td>\n",
              "      <td>0.703720</td>\n",
              "      <td>0.068767</td>\n",
              "      <td>0.076067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unigram Support Vector Classifier</th>\n",
              "      <td>0.878343</td>\n",
              "      <td>0.890966</td>\n",
              "      <td>0.862949</td>\n",
              "      <td>0.876725</td>\n",
              "      <td>6.782700</td>\n",
              "      <td>6.804633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bigram Support Vector Classifier</th>\n",
              "      <td>0.839261</td>\n",
              "      <td>0.859734</td>\n",
              "      <td>0.811840</td>\n",
              "      <td>0.835094</td>\n",
              "      <td>3.396033</td>\n",
              "      <td>3.408900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unigram Logistic Regression</th>\n",
              "      <td>0.878139</td>\n",
              "      <td>0.890733</td>\n",
              "      <td>0.862778</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>1.347467</td>\n",
              "      <td>1.356800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bigram Logistic Regression</th>\n",
              "      <td>0.839402</td>\n",
              "      <td>0.860919</td>\n",
              "      <td>0.810625</td>\n",
              "      <td>0.835008</td>\n",
              "      <td>0.758200</td>\n",
              "      <td>0.762967</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7a94f61-e799-4db9-a620-2a80dfc56764')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7a94f61-e799-4db9-a620-2a80dfc56764 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7a94f61-e799-4db9-a620-2a80dfc56764');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                   Accuracy  ...  Prediction Time\n",
              "Unigram Multinomial Naive Bayes    0.805131  ...         0.081700\n",
              "Bigram Multinomial Naive Bayes     0.659902  ...         0.076067\n",
              "Unigram Support Vector Classifier  0.878343  ...         6.804633\n",
              "Bigram Support Vector Classifier   0.839261  ...         3.408900\n",
              "Unigram Logistic Regression        0.878139  ...         1.356800\n",
              "Bigram Logistic Regression         0.839402  ...         0.762967\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "\n",
        "one_gram_mnb_tprs = []\n",
        "one_gram_mnb_aucs = []\n",
        "two_gram_mnb_tprs = []\n",
        "two_gram_mnb_aucs = []\n",
        "one_gram_svc_tprs = []\n",
        "one_gram_svc_aucs = []\n",
        "two_gram_svc_tprs = []\n",
        "two_gram_svc_aucs = []\n",
        "one_gram_log_tprs = []\n",
        "one_gram_log_aucs = []\n",
        "two_gram_log_tprs = []\n",
        "two_gram_log_aucs = []\n",
        "mean_fpr = np.linspace(0,1,100)\n",
        "\n",
        "one_gram_mnb_text = Pipeline([('one_gram_vectorizer', CountVectorizer(ngram_range=(1,1), max_features=150)),\n",
        "                     ('one_gram_tfidf', TfidfTransformer()),\n",
        "                     ('one_gram_mnb', MultinomialNB())])\n",
        "\n",
        "two_gram_mnb_text = Pipeline([('two_gram_vectorizer', CountVectorizer(ngram_range=(2,2), max_features=150)),\n",
        "                     ('two_gram_tfidf', TfidfTransformer()),\n",
        "                     ('two_gram_mnb', MultinomialNB())])\n",
        "\n",
        "one_gram_svc_text = Pipeline([('one_gram_vectorizer', CountVectorizer(ngram_range=(1,1), max_features=150)),\n",
        "                     ('one_gram_tfidf', TfidfTransformer()),\n",
        "                     ('one_gram_svc', CalibratedClassifierCV(svm.LinearSVC()))])\n",
        "\n",
        "two_gram_svc_text = Pipeline([('two_gram_vectorizer', CountVectorizer(ngram_range=(2,2), max_features=150)),\n",
        "                     ('two_gram_tfidf', TfidfTransformer()),\n",
        "                     ('two_gram_svc', CalibratedClassifierCV(svm.LinearSVC()))])\n",
        "\n",
        "one_gram_log_text = Pipeline([('one_gram_vectorizer', CountVectorizer(ngram_range=(1,1), max_features=150)),\n",
        "                     ('one_gram_tfidf', TfidfTransformer()),\n",
        "                     ('one_gram_log', LogisticRegression())])\n",
        "\n",
        "two_gram_log_text = Pipeline([('two_gram_vectorizer', CountVectorizer(ngram_range=(2,2), max_features=150)),\n",
        "                     ('two_gram_tfidf', TfidfTransformer()),\n",
        "                     ('two_gram_log', LogisticRegression())])\n",
        "\n",
        "for train_index, test_index in kfold.split(df_samples_text, df_samples_label):\n",
        "    x_train, x_test =  df_samples_text.iloc[train_index], df_samples_text.iloc[test_index]\n",
        "    y_train, y_test = df_samples_label.iloc[train_index], df_samples_label.iloc[test_index]\n",
        "    \n",
        "    one_gram_mnb_text.fit(x_train, y_train)\n",
        "    one_gram_mnb_probability = one_gram_mnb_text.predict_proba(x_test)\n",
        "    one_gram_mnb_predictions = one_gram_mnb_probability [:,1]\n",
        "    one_gram_mnb_fpr, one_gram_mnb_tpr, one_gram_mnb_threshold = metrics.roc_curve(y_test, one_gram_mnb_predictions)\n",
        "    one_gram_mnb_tprs.append(np.interp(mean_fpr, one_gram_mnb_fpr, one_gram_mnb_tpr))\n",
        "    one_gram_mnb_tprs[-1][0] = 0.0\n",
        "    one_gram_mnb_roc_auc = metrics.auc(one_gram_mnb_fpr, one_gram_mnb_tpr)\n",
        "    one_gram_mnb_aucs.append(one_gram_mnb_roc_auc)\n",
        "    plt.plot(one_gram_mnb_fpr, one_gram_mnb_tpr, lw=1, alpha=0.3)\n",
        "\n",
        "    two_gram_mnb_text.fit(x_train, y_train)\n",
        "    two_gram_mnb_probability = two_gram_mnb_text.predict_proba(x_test)\n",
        "    two_gram_mnb_predictions = two_gram_mnb_probability [:,1]\n",
        "    two_gram_mnb_fpr, two_gram_mnb_tpr, two_gram_mnb_threshold = metrics.roc_curve(y_test, two_gram_mnb_predictions)\n",
        "    two_gram_mnb_tprs.append(np.interp(mean_fpr, two_gram_mnb_fpr, two_gram_mnb_tpr))\n",
        "    two_gram_mnb_tprs[-1][0] = 0.0\n",
        "    two_gram_mnb_roc_auc = metrics.auc(two_gram_mnb_fpr, two_gram_mnb_tpr)\n",
        "    two_gram_mnb_aucs.append(two_gram_mnb_roc_auc)\n",
        "    plt.plot(two_gram_mnb_fpr, two_gram_mnb_tpr, lw=1, alpha=0.3)\n",
        "\n",
        "    one_gram_svc_text.fit(x_train, y_train)\n",
        "    one_gram_svc_probability = one_gram_svc_text.predict_proba(x_test)\n",
        "    one_gram_svc_predictions = one_gram_svc_probability [:,1]\n",
        "    one_gram_svc_fpr, one_gram_svc_tpr, one_gram_svc_threshold = metrics.roc_curve(y_test, one_gram_svc_predictions)\n",
        "    one_gram_svc_tprs.append(np.interp(mean_fpr, one_gram_svc_fpr, one_gram_svc_tpr))\n",
        "    one_gram_svc_tprs[-1][0] = 0.0\n",
        "    one_gram_svc_roc_auc = metrics.auc(one_gram_svc_fpr, one_gram_svc_tpr)\n",
        "    one_gram_svc_aucs.append(one_gram_svc_roc_auc)\n",
        "    plt.plot(one_gram_svc_fpr, one_gram_svc_tpr, lw=1, alpha=0.3)\n",
        "\n",
        "    two_gram_svc_text.fit(x_train, y_train)\n",
        "    two_gram_svc_probability = two_gram_svc_text.predict_proba(x_test)\n",
        "    two_gram_svc_predictions = two_gram_svc_probability [:,1]\n",
        "    two_gram_svc_fpr, two_gram_svc_tpr, two_gram_svc_threshold = metrics.roc_curve(y_test, two_gram_svc_predictions)\n",
        "    two_gram_svc_tprs.append(np.interp(mean_fpr, two_gram_svc_fpr, two_gram_svc_tpr))\n",
        "    two_gram_svc_tprs[-1][0] = 0.0\n",
        "    two_gram_svc_roc_auc = metrics.auc(two_gram_svc_fpr, two_gram_svc_tpr)\n",
        "    two_gram_svc_aucs.append(two_gram_svc_roc_auc)\n",
        "    plt.plot(two_gram_svc_fpr, two_gram_svc_tpr, lw=1, alpha=0.3)\n",
        "\n",
        "    one_gram_log_text.fit(x_train, y_train)\n",
        "    one_gram_log_probability = one_gram_log_text.predict_proba(x_test)\n",
        "    one_gram_log_predictions = one_gram_log_probability [:,1]\n",
        "    one_gram_log_fpr, one_gram_log_tpr, one_gram_log_threshold = metrics.roc_curve(y_test, one_gram_svc_predictions)\n",
        "    one_gram_log_tprs.append(np.interp(mean_fpr, one_gram_log_fpr, one_gram_log_tpr))\n",
        "    one_gram_log_tprs[-1][0] = 0.0\n",
        "    one_gram_log_roc_auc = metrics.auc(one_gram_log_fpr, one_gram_log_tpr)\n",
        "    one_gram_log_aucs.append(one_gram_log_roc_auc)\n",
        "    plt.plot(one_gram_log_fpr, one_gram_log_tpr, lw=1, alpha=0.3)\n",
        "\n",
        "    two_gram_log_text.fit(x_train, y_train)\n",
        "    two_gram_log_probability = two_gram_log_text.predict_proba(x_test)\n",
        "    two_gram_log_predictions = two_gram_log_probability [:,1]\n",
        "    two_gram_log_fpr, two_gram_log_tpr, two_gram_log_threshold = metrics.roc_curve(y_test, two_gram_log_predictions)\n",
        "    two_gram_log_tprs.append(np.interp(mean_fpr, two_gram_log_fpr, two_gram_log_tpr))\n",
        "    two_gram_log_tprs[-1][0] = 0.0\n",
        "    two_gram_log_roc_auc = metrics.auc(two_gram_log_fpr, two_gram_log_tpr)\n",
        "    two_gram_log_aucs.append(two_gram_log_roc_auc)\n",
        "    plt.plot(two_gram_log_fpr, two_gram_log_tpr, lw=1, alpha=0.3)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         label='Chance', alpha=.8)\n",
        "\n",
        "one_gram_mnb_mean_tpr = np.mean(one_gram_mnb_tprs, axis=0)\n",
        "one_gram_mnb_mean_tpr[-1] = 1.0\n",
        "one_gram_mnb_mean_auc = metrics.auc(mean_fpr, one_gram_mnb_mean_tpr)\n",
        "one_gram_mnb_std_auc = np.std(one_gram_mnb_aucs)\n",
        "plt.plot(mean_fpr, one_gram_mnb_mean_tpr, color='aqua',\n",
        "         label=r'Unigram MNB; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (one_gram_mnb_mean_auc, one_gram_mnb_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "one_gram_mnb_std_tpr = np.std(one_gram_mnb_tprs, axis=0)\n",
        "one_gram_mnb_tprs_upper = np.minimum(one_gram_mnb_mean_tpr + one_gram_mnb_std_tpr, 1)\n",
        "one_gram_mnb_tprs_lower = np.maximum(one_gram_mnb_mean_tpr - one_gram_mnb_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, one_gram_mnb_tprs_lower, one_gram_mnb_tprs_upper, color='grey', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "two_gram_mnb_mean_tpr = np.mean(two_gram_mnb_tprs, axis=0)\n",
        "two_gram_mnb_mean_tpr[-1] = 1.0\n",
        "two_gram_mnb_mean_auc = metrics.auc(mean_fpr, two_gram_mnb_mean_tpr)\n",
        "two_gram_mnb_std_auc = np.std(two_gram_mnb_aucs)\n",
        "plt.plot(mean_fpr, two_gram_mnb_mean_tpr, color='blue',\n",
        "         label=r'Bigram MNB; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (two_gram_mnb_mean_auc, two_gram_mnb_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "two_gram_mnb_std_tpr = np.std(two_gram_mnb_tprs, axis=0)\n",
        "two_gram_mnb_tprs_upper = np.minimum(two_gram_mnb_mean_tpr + two_gram_mnb_std_tpr, 1)\n",
        "two_gram_mnb_tprs_lower = np.maximum(two_gram_mnb_mean_tpr - two_gram_mnb_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, two_gram_mnb_tprs_lower, two_gram_mnb_tprs_upper, color='whitesmoke', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "one_gram_svc_mean_tpr = np.mean(one_gram_svc_tprs, axis=0)\n",
        "one_gram_svc_mean_tpr[-1] = 1.0\n",
        "one_gram_svc_mean_auc = metrics.auc(mean_fpr, one_gram_svc_mean_tpr)\n",
        "one_gram_svc_std_auc = np.std(one_gram_svc_aucs)\n",
        "plt.plot(mean_fpr, one_gram_svc_mean_tpr, color='lime',\n",
        "         label=r'Unigram SVC; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (one_gram_svc_mean_auc, one_gram_svc_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "one_gram_svc_std_tpr = np.std(one_gram_svc_tprs, axis=0)\n",
        "one_gram_svc_tprs_upper = np.minimum(one_gram_svc_mean_tpr + one_gram_svc_std_tpr, 1)\n",
        "one_gram_svc_tprs_lower = np.maximum(one_gram_svc_mean_tpr - one_gram_svc_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, one_gram_svc_tprs_lower, one_gram_svc_tprs_upper, color='chocolate', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "two_gram_svc_mean_tpr = np.mean(two_gram_svc_tprs, axis=0)\n",
        "two_gram_svc_mean_tpr[-1] = 1.0\n",
        "two_gram_svc_mean_auc = metrics.auc(mean_fpr, two_gram_svc_mean_tpr)\n",
        "two_gram_svc_std_auc = np.std(two_gram_svc_aucs)\n",
        "plt.plot(mean_fpr, two_gram_svc_mean_tpr, color='darkgreen',\n",
        "         label=r'Bigram SVC; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (two_gram_svc_mean_auc, two_gram_svc_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "two_gram_svc_std_tpr = np.std(two_gram_svc_tprs, axis=0)\n",
        "two_gram_svc_tprs_upper = np.minimum(two_gram_svc_mean_tpr + two_gram_svc_std_tpr, 1)\n",
        "two_gram_svc_tprs_lower = np.maximum(two_gram_svc_mean_tpr - two_gram_svc_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, two_gram_svc_tprs_lower, two_gram_svc_tprs_upper, color='lightsalmon', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "one_gram_log_mean_tpr = np.mean(one_gram_log_tprs, axis=0)\n",
        "one_gram_log_mean_tpr[-1] = 1.0\n",
        "one_gram_log_mean_auc = metrics.auc(mean_fpr, one_gram_log_mean_tpr)\n",
        "one_gram_log_std_auc = np.std(one_gram_log_aucs)\n",
        "plt.plot(mean_fpr, one_gram_log_mean_tpr, color='gold',\n",
        "         label=r'Unigram Log;Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (one_gram_log_mean_auc, one_gram_log_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "one_gram_log_std_tpr = np.std(one_gram_log_tprs, axis=0)\n",
        "one_gram_log_tprs_upper = np.minimum(one_gram_log_mean_tpr + one_gram_log_std_tpr, 1)\n",
        "one_gram_log_tprs_lower = np.maximum(one_gram_log_mean_tpr - one_gram_log_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, one_gram_log_tprs_lower, one_gram_log_tprs_upper, color='indigo', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "two_gram_log_mean_tpr = np.mean(two_gram_log_tprs, axis=0)\n",
        "two_gram_log_mean_tpr[-1] = 1.0\n",
        "two_gram_log_mean_auc = metrics.auc(mean_fpr, two_gram_log_mean_tpr)\n",
        "two_gram_log_std_auc = np.std(two_gram_svc_aucs)\n",
        "plt.plot(mean_fpr, one_gram_log_mean_tpr, color='orange',\n",
        "         label=r'Bigram Log;Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (two_gram_log_mean_auc, two_gram_log_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "two_gram_log_std_tpr = np.std(two_gram_log_tprs, axis=0)\n",
        "two_gram_log_tprs_upper = np.minimum(two_gram_log_mean_tpr + two_gram_log_std_tpr, 1)\n",
        "two_gram_log_tprs_lower = np.maximum(two_gram_log_mean_tpr - two_gram_log_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, two_gram_log_tprs_lower, two_gram_log_tprs_upper, color='violet', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Cross-Validation ROC of Implemented Algorithms')\n",
        "plt.legend(loc=\"below\", prop={'size': 9})\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "pROISBoMPjXJ",
        "outputId": "bb7c29bf-ee55-4326-f100-3a92f446d0e4"
      },
      "id": "pROISBoMPjXJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-27be8da180bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_gram_svc_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_gram_svc_tpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mtwo_gram_svc_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mtwo_gram_svc_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_gram_svc_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mtwo_gram_svc_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_gram_svc_probability\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             )\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                     \u001b[0;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
        "         label='Chance', alpha=.8)\n",
        "\n",
        "one_gram_mnb_mean_tpr = np.mean(one_gram_mnb_tprs, axis=0)\n",
        "one_gram_mnb_mean_tpr[-1] = 1.0\n",
        "one_gram_mnb_mean_auc = metrics.auc(mean_fpr, one_gram_mnb_mean_tpr)\n",
        "one_gram_mnb_std_auc = np.std(one_gram_mnb_aucs)\n",
        "plt.plot(mean_fpr, one_gram_mnb_mean_tpr, color='b',\n",
        "         label=r'Unigram MNB; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (one_gram_mnb_mean_auc, one_gram_mnb_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "one_gram_mnb_std_tpr = np.std(one_gram_mnb_tprs, axis=0)\n",
        "one_gram_mnb_tprs_upper = np.minimum(one_gram_mnb_mean_tpr + one_gram_mnb_std_tpr, 1)\n",
        "one_gram_mnb_tprs_lower = np.maximum(one_gram_mnb_mean_tpr - one_gram_mnb_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, one_gram_mnb_tprs_lower, one_gram_mnb_tprs_upper, color='grey', alpha=.2)\n",
        "\n",
        "two_gram_mnb_mean_tpr = np.mean(two_gram_mnb_tprs, axis=0)\n",
        "two_gram_mnb_mean_tpr[-1] = 1.0\n",
        "two_gram_mnb_mean_auc = metrics.auc(mean_fpr, two_gram_mnb_mean_tpr)\n",
        "two_gram_mnb_std_auc = np.std(two_gram_mnb_aucs)\n",
        "plt.plot(mean_fpr, two_gram_mnb_mean_tpr, color='g',\n",
        "         label=r'Bigram MNB; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (two_gram_mnb_mean_auc, two_gram_mnb_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "two_gram_mnb_std_tpr = np.std(two_gram_mnb_tprs, axis=0)\n",
        "two_gram_mnb_tprs_upper = np.minimum(two_gram_mnb_mean_tpr + two_gram_mnb_std_tpr, 1)\n",
        "two_gram_mnb_tprs_lower = np.maximum(two_gram_mnb_mean_tpr - two_gram_mnb_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, two_gram_mnb_tprs_lower, two_gram_mnb_tprs_upper, color='whitesmoke', alpha=.2)\n",
        "\n",
        "one_gram_svc_mean_tpr = np.mean(one_gram_svc_tprs, axis=0)\n",
        "one_gram_svc_mean_tpr[-1] = 1.0\n",
        "one_gram_svc_mean_auc = metrics.auc(mean_fpr, one_gram_svc_mean_tpr)\n",
        "one_gram_svc_std_auc = np.std(one_gram_svc_aucs)\n",
        "plt.plot(mean_fpr, one_gram_svc_mean_tpr, color='r',\n",
        "         label=r'Unigram SVC; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (one_gram_svc_mean_auc, one_gram_svc_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "one_gram_svc_std_tpr = np.std(one_gram_svc_tprs, axis=0)\n",
        "one_gram_svc_tprs_upper = np.minimum(one_gram_svc_mean_tpr + one_gram_svc_std_tpr, 1)\n",
        "one_gram_svc_tprs_lower = np.maximum(one_gram_svc_mean_tpr - one_gram_svc_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, one_gram_svc_tprs_lower, one_gram_svc_tprs_upper, color='chocolate', alpha=.2)\n",
        "\n",
        "two_gram_svc_mean_tpr = np.mean(two_gram_svc_tprs, axis=0)\n",
        "two_gram_svc_mean_tpr[-1] = 1.0\n",
        "two_gram_svc_mean_auc = metrics.auc(mean_fpr, two_gram_svc_mean_tpr)\n",
        "two_gram_svc_std_auc = np.std(two_gram_svc_aucs)\n",
        "plt.plot(mean_fpr, two_gram_svc_mean_tpr, color='c',\n",
        "         label=r'Bigram SVC; Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (two_gram_svc_mean_auc, two_gram_svc_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "two_gram_svc_std_tpr = np.std(two_gram_svc_tprs, axis=0)\n",
        "two_gram_svc_tprs_upper = np.minimum(two_gram_svc_mean_tpr + two_gram_svc_std_tpr, 1)\n",
        "two_gram_svc_tprs_lower = np.maximum(two_gram_svc_mean_tpr - two_gram_svc_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, two_gram_svc_tprs_lower, two_gram_svc_tprs_upper, color='lightsalmon', alpha=.2)\n",
        "\n",
        "one_gram_log_mean_tpr = np.mean(one_gram_log_tprs, axis=0)\n",
        "one_gram_log_mean_tpr[-1] = 1.0\n",
        "one_gram_log_mean_auc = metrics.auc(mean_fpr, one_gram_log_mean_tpr)\n",
        "one_gram_log_std_auc = np.std(one_gram_log_aucs)\n",
        "plt.plot(mean_fpr, one_gram_log_mean_tpr, color='m',\n",
        "         label=r'Unigram Log;Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (one_gram_log_mean_auc, one_gram_log_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "one_gram_log_std_tpr = np.std(one_gram_log_tprs, axis=0)\n",
        "one_gram_log_tprs_upper = np.minimum(one_gram_log_mean_tpr + one_gram_log_std_tpr, 1)\n",
        "one_gram_log_tprs_lower = np.maximum(one_gram_log_mean_tpr - one_gram_log_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, one_gram_log_tprs_lower, one_gram_log_tprs_upper, color='indigo', alpha=.2)\n",
        "\n",
        "two_gram_log_mean_tpr = np.mean(two_gram_log_tprs, axis=0)\n",
        "two_gram_log_mean_tpr[-1] = 1.0\n",
        "two_gram_log_mean_auc = metrics.auc(mean_fpr, two_gram_log_mean_tpr)\n",
        "two_gram_log_std_auc = np.std(two_gram_svc_aucs)\n",
        "plt.plot(mean_fpr, one_gram_log_mean_tpr, color='y',\n",
        "         label=r'Bigram Log;Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (two_gram_log_mean_auc, two_gram_log_std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "two_gram_log_std_tpr = np.std(two_gram_log_tprs, axis=0)\n",
        "two_gram_log_tprs_upper = np.minimum(two_gram_log_mean_tpr + two_gram_log_std_tpr, 1)\n",
        "two_gram_log_tprs_lower = np.maximum(two_gram_log_mean_tpr - two_gram_log_std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, two_gram_log_tprs_lower, two_gram_log_tprs_upper, color='violet', alpha=.2)\n",
        "\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Cross-Validation ROC of Implemented Algorithms')\n",
        "plt.legend(loc=\"below\", prop={'size': 9})\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "v61q4ZQyrYlu",
        "outputId": "beba8f81-8050-4b23-afa1-b3e9419dc1cb"
      },
      "id": "v61q4ZQyrYlu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fssaul-bWSF2"
      ],
      "name": "Roost Final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}